<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Human Video Generation - ImportantArticles</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Important Articles">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">动画3D管线 - 3DMesh的驱动</li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> 基于骨骼代理的Mesh的驱动</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../MeshAnimation/SkeletonProxy/MotionPrior.html"><strong aria-hidden="true">2.1.</strong> 骨骼动作先验</a></li><li class="chapter-item expanded "><a href="../MeshAnimation/SkeletonProxy/MotionGenerationDiscreteRepresentation.html"><strong aria-hidden="true">2.2.</strong> 基于离散表示的骨骼动作生成</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.</strong> 骨骼动作生成</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../MeshAnimation/SkeletonProxy/MotionGeneration/DiffusionBasedText2Motion.html"><strong aria-hidden="true">2.3.1.</strong> 基于Diffusion的文生动作</a></li></ol></li><li class="chapter-item expanded "><a href="../CharacterAnimation/HPE_HMR_Summary.html"><strong aria-hidden="true">2.4.</strong> 3D Human Pose Estimation and Mesh Recovery</a></li><li class="chapter-item expanded "><a href="../CharacterAnimation/HumanFacialAnimation.html"><strong aria-hidden="true">2.5.</strong> facial and expression</a></li><li class="chapter-item expanded "><a href="../CharacterAnimation/HumanMotionGenerationSummary.html"><strong aria-hidden="true">2.6.</strong> Human Motion Generation: A Survey</a></li></ol></li><li class="chapter-item expanded "><a href="../MeshAnimation/E2E.html"><strong aria-hidden="true">3.</strong> 无代理的Mesh驱动</a></li><li class="chapter-item expanded affix "><li class="part-title">动画3D管线 - 3DGS的驱动</li><li class="chapter-item expanded "><a href="../3D_Gaussian_Splatting.html"><strong aria-hidden="true">4.</strong> A Survey on 3D Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="../CharacterAnimation/Human4DGeneration.html"><strong aria-hidden="true">5.</strong> Human 4D Generation</a></li><li class="chapter-item expanded "><a href="../4DGeneration.html"><strong aria-hidden="true">6.</strong> 4D Generation</a></li><li class="chapter-item expanded "><a href="../AnimationGeneration.html"><strong aria-hidden="true">7.</strong> Animal Generation</a></li><li class="chapter-item expanded affix "><li class="part-title">动画2D管线 - 像素的驱动，可控视频生成</li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/Introduction.html"><strong aria-hidden="true">8.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/VideoGeneration.html"><strong aria-hidden="true">9.</strong> Video Generation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/Pioneeringearlyworks.html"><strong aria-hidden="true">9.1.</strong> 闭源T2V大模型</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/Open-sourcebasemodels.html"><strong aria-hidden="true">9.2.</strong> 开源T2V基模型</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/WorksBasedOnT2I.html"><strong aria-hidden="true">9.3.</strong> Works Based on T2I 基模型</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/WorksBasedOnT2V.html"><strong aria-hidden="true">9.4.</strong> Works Based on T2V 基模型</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/Storyboard.html"><strong aria-hidden="true">9.5.</strong> Storyboard</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/Longvideogeneration.html"><strong aria-hidden="true">9.6.</strong> Long video generation/Storyboard</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoGeneration/Multimodal-guidedgeneration.html"><strong aria-hidden="true">9.7.</strong> Multimodal-guided generation</a></li><li class="chapter-item expanded "><a href="../CharacterAnimation/HumanVideoGeneration.html" class="active"><strong aria-hidden="true">9.8.</strong> Human Video Generation</a></li></ol></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing.html"><strong aria-hidden="true">10.</strong> Video Editing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing/Tuning-based.html"><strong aria-hidden="true">10.1.</strong> Tuning-based</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing/Training-free.html"><strong aria-hidden="true">10.2.</strong> Training-free</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing/ControlledEditing.html"><strong aria-hidden="true">10.3.</strong> Controlled Editing</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing/3D-Aware.html"><strong aria-hidden="true">10.4.</strong> 3D-Aware</a></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/VideoEditing/OtherGuidance.html"><strong aria-hidden="true">10.5.</strong> Other Guidance</a></li></ol></li><li class="chapter-item expanded "><a href="../VideoDiffusionModels/EvaluationMetrics.html"><strong aria-hidden="true">11.</strong> 评价指标</a></li><li class="chapter-item expanded affix "><li class="part-title">动画2D管线 - 2D图形的驱动</li><li class="chapter-item expanded "><a href="../ClipAnimation.html"><strong aria-hidden="true">12.</strong> 2D图形驱动</a></li><li class="chapter-item expanded affix "><li class="part-title">通用AI技术</li><li class="chapter-item expanded "><a href="../NeurIPS2024FlowMatchigTurorial/Agenda.html"><strong aria-hidden="true">13.</strong> NeurIPS 2024 Flow Matchig Turorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../NeurIPS2024FlowMatchigTurorial/FlowMatchingBasics.html"><strong aria-hidden="true">13.1.</strong> Flow Matching Basics</a></li><li class="chapter-item expanded "><a href="../NeurIPS2024FlowMatchigTurorial/FlowMatchingAdvancedDesigns.html"><strong aria-hidden="true">13.2.</strong> Flow Matching Advanced Designs</a></li><li class="chapter-item expanded "><a href="../NeurIPS2024FlowMatchigTurorial/ModelAdaptation.html"><strong aria-hidden="true">13.3.</strong> Model Adaptation</a></li><li class="chapter-item expanded "><a href="../NeurIPS2024FlowMatchigTurorial/GeneratorMatchingandDiscreteFlows.html"><strong aria-hidden="true">13.4.</strong> Generator Matching and Discrete Flows</a></li></ol></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Introduction.html"><strong aria-hidden="true">14.</strong> Introduction</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">15.</strong> Fundamentals</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Fundamentals/DenoisingDiffusionProbabilisticModels.html"><strong aria-hidden="true">15.1.</strong> Denoising Diffusion Probabilistic Models</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Fundamentals/Score-basedGenerativeModelingwithDifferentialEquations.html"><strong aria-hidden="true">15.2.</strong> Score-based Generative Modeling with Differential Equations</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Fundamentals/AcceleratedSampling.html"><strong aria-hidden="true">15.3.</strong> Accelerated Sampling</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Fundamentals/ConditionalGenerationandGuidance.html"><strong aria-hidden="true">15.4.</strong> Conditional Generation and Guidance</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Fundamentals/Summary.html"><strong aria-hidden="true">15.5.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/Architecture.html"><strong aria-hidden="true">16.</strong> T2I 基模型</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">17.</strong> Image Applications Based on 基模型</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationOnImage/ImageEditing.html"><strong aria-hidden="true">17.1.</strong> 图像生成/编辑</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationOnImage/InverseProblems.html"><strong aria-hidden="true">17.2.</strong> 图像去噪/图像超分/图像补全</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationOnImage/LargeContents.html"><strong aria-hidden="true">17.3.</strong> 大图生成</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">18.</strong> 3D Applications Based on Diffusion</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/2Ddiffusionmodelsfor3Dgeneration.html"><strong aria-hidden="true">18.1.</strong> 基于T2I基模型</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/3D.html"><strong aria-hidden="true">18.2.</strong> 基于不同视角的3D生成</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/Diffusionmodelsforviewsynthesis.html"><strong aria-hidden="true">18.3.</strong> 新视角合成</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/3Dreconstruction.html"><strong aria-hidden="true">18.4.</strong> 3D重建</a></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/Inverseproblems.html"><strong aria-hidden="true">18.5.</strong> 3D编辑</a></li></ol></li><li class="chapter-item expanded "><a href="../diffusion-tutorial-part/ApplicationsOn3D/Safetyandlimitationsofdiffusionmodels.html"><strong aria-hidden="true">19.</strong> Safety and limitations of diffusion models</a></li><li class="chapter-item expanded "><a href="../LargeMultimodalModelsNotesonCVPR2023Tutorial.html"><strong aria-hidden="true">20.</strong> Large Multimodal Models Notes on CVPR 2023 Tutorial</a></li><li class="chapter-item expanded "><a href="../GenerativeModels.html"><strong aria-hidden="true">21.</strong> 生成模型</a></li><li class="chapter-item expanded affix "><li class="part-title">Others</li><li class="chapter-item expanded "><a href="../数据集.html"><strong aria-hidden="true">22.</strong> 数据集</a></li><li class="chapter-item expanded "><a href="../More.html"><strong aria-hidden="true">23.</strong> More</a></li><li class="chapter-item expanded affix "><li class="part-title">Views</li><li class="chapter-item expanded "><a href="../Views/20250903.html"><strong aria-hidden="true">24.</strong> 2025.9.3骨骼动作生成</a></li><li class="chapter-item expanded "><a href="../Views/20250914.html"><strong aria-hidden="true">25.</strong> 2025.9.14骨骼动作离散编码</a></li><li class="chapter-item expanded "><a href="../Views/20250920.html"><strong aria-hidden="true">26.</strong> 2025.9.20视频可控生成</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ImportantArticles</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ImportantArticles" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_1_Figure_1_37170738.png" alt="" /></p>
<h1 id="人类视频生成的基础知"><a class="header" href="#人类视频生成的基础知">人类视频生成的基础知</a></h1>
<h1 id="关键子任务"><a class="header" href="#关键子任务">关键子任务</a></h1>
<p>根据驱动生成过程的模态将现有方法分为三类：文本驱动、音频驱动和姿势驱动</p>
<h2 id="文本驱动的人类视频生成"><a class="header" href="#文本驱动的人类视频生成">文本驱动的人类视频生成</a></h2>
<p>讨论了如何使用文本描述来控制生成视频中的人类外观和动作。</p>
<p><img src="../assets/c5094236dee05a597cc12eb2a5b13473_4_Figure_3_-1248124106.png" alt="" /></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.5.21</td><td>Interspatial Attention for Efficient 4D Human Video Generation</td><td></td><td>以可控方式生成数字人(digital humans)的逼真视频</td><td><a href="106.html">link</a></td></tr>
<tr><td>1</td><td>2024</td><td>ID-Animator</td><td>To ensure the consistency of appearance in generated videos with the textual descriptions while preserving identity details during frames, ID-Animator [1] leverages a pre-trained textto-video (T2V) model with a lightweight face adapter to encode identity-relevant embeddings.</td><td>人体外观控制</td><td></td></tr>
<tr><td>83</td><td></td><td>HMTV</td><td>文本生成动作和相机运动，再生成图像</td><td>人体动作控制，2阶段方法</td><td></td></tr>
<tr><td>84</td><td>2020</td><td>SignSynth</td><td>Gloss2Pose文生动作，GAN动作生视频</td><td>人体动作控制，2阶段方法</td><td></td></tr>
<tr><td>85</td><td>2022</td><td>H-DNA</td><td></td><td>人体动作控制，2阶段方法</td><td></td></tr>
<tr><td>86</td><td>2024</td><td>SignLLM</td><td>文本-&gt;GLoss-&gt;Pose-&gt;Video</td><td>人体动作控制，2阶段方法</td><td></td></tr>
<tr><td>89</td><td>2024</td><td></td><td>文本-&gt;GLoss-&gt;Pose-&gt;Video</td><td>人体动作控制，2阶段方法</td><td></td></tr>
<tr><td>53</td><td></td><td>Text2Performer</td><td>involves the motion text and a motion encoder. motion text describes the movement, such as &quot;She is swinging to the right.&quot; The model implicitly models these descriptions by separately representing appearance and motion, thereby generating high-quality videos with consistent appearance and actions.</td><td>text作为prompt直接生成video</td><td></td></tr>
</tbody></table>
<h2 id="音频驱动的人类视频生成"><a class="header" href="#音频驱动的人类视频生成">音频驱动的人类视频生成</a></h2>
<p>语音驱动：要求生成的人体动作在高级语义方面及在情感和节奏方面与音频和谐。<br />
音乐驱动：合成一个人在给定的音乐片段引导下跳舞或演奏某种乐器的视频，关注于低级节拍对齐。</p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_5_Figure_4_581747205.png" alt="" /><br />
<img src="./assets/c5094236dee05a597cc12eb2a5b13473_5_Table_III_-1681426925.png" alt="" /></p>
<h3 id="语音驱动手势"><a class="header" href="#语音驱动手势">语音驱动手势</a></h3>
<p>以下是整理后的表格，概述了<strong>语音驱动人体视频生成模型</strong>的关键特性与演进：</p>
<hr />
<table><thead><tr><th><strong>方法/模型</strong></th><th><strong>基础架构</strong></th><th><strong>主要贡献</strong></th><th><strong>输入</strong></th><th><strong>输出</strong></th><th><strong>训练目标/优化方法</strong></th><th><strong>关键创新点</strong></th><th><strong>局限性</strong></th></tr></thead><tbody>
<tr><td><strong>传统方法 [61][92][93]</strong></td><td>2D/3D骨架 + 分离式渲染</td><td>基于结构先验（骨架）生成手势视频</td><td>语音 + 2D/3D骨架</td><td>手势视频</td><td>骨架运动生成与视频渲染分离</td><td>利用手写结构先验（骨架）定义运动</td><td>外观信息丢失，控制困难；预训练姿态估计器导致抖动与误差累积</td></tr>
<tr><td><strong>ANGIE [62]</strong></td><td>无监督MRAA特征 + VQ-VAE + GPT网络</td><td>通过无监督特征与离散化建模提升手势生成</td><td>语音</td><td>手势视频</td><td>VQ-VAE量化运动模式 + 自回归预测离散动作</td><td>无监督运动特征（MRAA）避免依赖骨架标注</td><td>MRAA线性建模限制复杂区域表达；语音与协方差关联不准确</td></tr>
<tr><td><strong>DiffTED &amp; He et al.</strong></td><td>TPS运动模型 + 扩散模型</td><td>解耦运动与外观，保留身体区域关键信息</td><td>语音 + TPS关键点</td><td>多样化手势视频</td><td>扩散模型生成运动序列 + TPS渲染关键点至图像</td><td>基于扩散的多样化生成；解耦运动与外观（避免信息丢失）</td><td>依赖TPS模型精度；计算成本较高</td></tr>
</tbody></table>
<hr />
<h4 id="关键说明"><a class="header" href="#关键说明"><strong>关键说明</strong></a></h4>
<ol>
<li><strong>技术演进</strong>：
<ul>
<li><strong>传统方法</strong>依赖刚性骨架，导致外观信息丢失与抖动问题；</li>
<li><strong>ANGIE</strong>引入无监督特征与离散化建模，但受限于线性表达能力；</li>
<li><strong>DiffTED</strong>通过解耦运动与外观、结合扩散模型，实现高质量多样化生成。</li>
</ul>
</li>
<li><strong>核心挑战</strong>：
<ul>
<li><strong>运动-外观平衡</strong>：传统方法牺牲外观信息，DiffTED通过解耦部分保留；</li>
<li><strong>生成多样性</strong>：扩散模型（DiffTED）优于自回归（ANGIE）与骨架驱动方法。</li>
</ul>
</li>
<li><strong>未来方向</strong>：
<ul>
<li>结合物理仿真优化运动真实性（如减少抖动）；</li>
<li>提升复杂区域（手部、微表情）的细粒度控制能力。</li>
</ul>
</li>
</ol>
<p>此表格对比了语音驱动手势视频生成的关键方法，凸显从结构先验到无监督学习再到解耦扩散模型的技术路径。</p>
<h3 id="语音驱动口型视频生成"><a class="header" href="#语音驱动口型视频生成">语音驱动口型(视频生成)</a></h3>
<p>唇形同步技术需要根据输入的音频生成对应的唇部动作，同时保持头部姿态和人物身份的一致性。</p>
<p>Image + Audio -&gt; Video</p>
<p>以下是整理后的表格，概述了<strong>音频驱动说话人脸生成方法</strong>的分类、核心特性与挑战：</p>
<hr />
<table><thead><tr><th><strong>方法类型</strong></th><th><strong>关键方法/技术</strong></th><th><strong>输入</strong></th><th><strong>输出</strong></th><th><strong>优点</strong></th><th><strong>局限性</strong></th></tr></thead><tbody>
<tr><td><strong>Person-Specific</strong></td><td>3D模型（Song et al., 2020; Thies et al., 2020）<br>NeRF（Park et al., 2022）</td><td>音频 + 目标人物多分钟训练视频</td><td>高保真、身份保留的说话视频</td><td>高保真，精确的唇部-音频映射</td><td>训练耗时，依赖大量目标数据，难以实时应用</td></tr>
<tr><td><strong>One-Shot Talking Head</strong></td><td>两阶段流程（音频→标志→视频，Chen et al., 2019）<br>3D系数驱动（Chen et al., 2020）</td><td>音频 + 单张参考图像</td><td>多样化表情与头部运动的视频</td><td>单图驱动，灵活性强；扩散模型（Tian et al., 2024）提升生成多样性</td><td>细节缺失（牙齿/纹理）；扩散模型导致身份细节丢失、计算成本高、推理步骤复杂</td></tr>
<tr><td><strong>Few-Shot Face Visual Dubbing</strong></td><td>编码器-解码器（Prajwal et al., 2020a）<br>变形修复网络（Zhang et al., 2023）</td><td>音频 + 源人脸（少量参考图）</td><td>嘴部替换的配音视频</td><td>直接替换唇部区域，适配性强</td><td>纹理模糊、身份不一致；修复网络易过拟合，局部颜色差异</td></tr>
</tbody></table>
<hr />
<h4 id="关键说明-1"><a class="header" href="#关键说明-1"><strong>关键说明</strong></a></h4>
<ol>
<li><strong>输入需求差异</strong>：
<ul>
<li><strong>Person-Specific</strong>：依赖目标人物大量训练数据；</li>
<li><strong>One-Shot</strong>：仅需单张参考图，灵活性高；</li>
<li><strong>Few-Shot</strong>：基于少量参考图进行局部（嘴部）替换。</li>
</ul>
</li>
<li><strong>核心挑战</strong>：
<ul>
<li><strong>保真度与效率</strong>：Person-Specific保真但低效，One-Shot/Diffusion多样但计算昂贵；</li>
<li><strong>细节保留</strong>：牙齿、嘴部纹理与高频细节仍是技术瓶颈（尤其One-Shot与Few-Shot）。</li>
</ul>
</li>
<li><strong>代表工作演进</strong>：
<ul>
<li><strong>3D模型 → 扩散模型</strong>：从基于物理建模转向生成式AI，提升多样性但牺牲确定性；</li>
<li><strong>编码器-解码器 → 变形修复</strong>：Few-Shot方法逐步优化纹理保留，但仍需解决过拟合问题。</li>
</ul>
</li>
</ol>
<p>此表格对比了音频驱动说话人脸生成的核心方法类型，凸显其在不同应用场景下的优势与待突破点。</p>
<hr />
<h3 id="唇音同步视频编辑"><a class="header" href="#唇音同步视频编辑">唇音同步(视频编辑)</a></h3>
<p>Video + Audio -&gt; Video</p>
<ul>
<li>扩散模型（如[29]）在细节丰富度上占优，但生成速度较慢；</li>
<li>GAN类方法（如MuseTalk）牺牲部分细节以提升速度。</li>
</ul>
<h4 id="基于扩散模型的唇音同步方法"><a class="header" href="#基于扩散模型的唇音同步方法"><strong>基于扩散模型的唇音同步方法</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>89</td><td>2025.3.13</td><td>LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision</td><td>1. 在latent space训练，在pixel space监督<br> 2. 用TREPA代表temporal layer<br> 3. 系统性地分析SyncNet的训练参数与效果</td><td>LDM, 开源</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/89.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</td><td>&gt; ✅（1）把说话的部分 mask 掉 （2）用 diffusion 根据 Audio Feature 生成说话的部分。<br> ✅ 额外约束：（1）reference 状态 （2）前后帧 smooth <br> ✅ 语音驱动嘴形。</td><td><img src="./../assets/08-267.png" alt="" /></td><td></td></tr>
</tbody></table>
<table><thead><tr><th><strong>方法/论文</strong></th><th><strong>关键架构</strong></th><th><strong>训练策略</strong></th><th><strong>生成阶段说明</strong></th><th><strong>输入 → 输出</strong></th><th><strong>主要创新点</strong></th></tr></thead><tbody>
<tr><td><strong>[34] &amp; [2]</strong>  2024</td><td>像素空间扩散模型</td><td>端到端音频条件扩散</td><td>单阶段：直接生成同步唇部图像</td><td>音频 → 图像</td><td>端到端像素级扩散，无需中间表示</td></tr>
<tr><td><strong>[57]</strong>     2024</td><td>扩散模型 + VAE</td><td>两阶段训练</td><td>阶段1：扩散模型（音频→运动）<br>阶段2：VAE（运动→图像）</td><td>音频 → 运动 → 图像</td><td>分阶段解耦运动与渲染，降低生成复杂度</td></tr>
<tr><td><strong>[64]</strong>   2024.08</td><td>Transformer + 扩散模型</td><td>两阶段训练</td><td>阶段1：Transformer（音频→运动）<br>阶段2：扩散模型（运动→图像）</td><td>音频 → 运动 → 图像</td><td>Transformer编码音频时序，扩散模型细化生成</td></tr>
<tr><td><strong>[29]</strong>    2024</td><td>扩散自编码器</td><td>两阶段训练</td><td>阶段1：扩散自编码器（掩码图→语义代码）<br>阶段2：扩散模型（语义代码+音频→图像）</td><td>音频 + 掩码图 → 图像</td><td>结合语义潜在代码与音频条件，增强可控性</td></tr>
</tbody></table>
<hr />
<h4 id="非扩散模型的唇音同步方法"><a class="header" href="#非扩散模型的唇音同步方法"><strong>非扩散模型的唇音同步方法</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.6.17</td><td>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</td><td></td><td></td><td><a href="199.html">link</a></td></tr>
<tr><td>90</td><td>2024.10</td><td>MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting</td><td>1. 借用扩散架构但采用GAN式训练（无扩散过程），平衡生成速度与质量<br> 2. 用根据特征筛选的方式选择reference image，提升生成质量。</td><td>LDM, 开源，实时，GAN, 逐帧, VQ-VAE</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/90.html">link</a></td></tr>
<tr><td>91</td><td>2020.8.23</td><td>A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</td><td>1. 首个跨ID的唇间同步口型生成方法<br>2. 预训练唇同步判别器（SyncNet监督） + 对抗学习<br>3. 提出唇音对齐性指标LSE-C和LSE-D</td><td>Wav2Lip， GAN, SyncNet, LSE-C, LSE-D</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/91.html">link</a></td></tr>
</tbody></table>
<p>| SyncNet监督生成器                     |      | 单阶段：生成器直接输出唇部同步视频                                              | 音频 → 图像            | 引入SyncNet作为判别器，提升唇部同步精度                                         |</p>
<table><thead><tr><th><strong>方法/论文</strong></th><th><strong>关键架构</strong></th><th><strong>训练策略</strong></th><th><strong>生成阶段说明</strong></th><th><strong>输入 → 输出</strong></th><th><strong>主要创新点</strong></th></tr></thead><tbody>
<tr><td><strong>[20]</strong>    2023</td><td>VQ-VAE + 量化空间生成器</td><td>分阶段训练</td><td>阶段1：VQ-VAE编码面部/头部姿势<br>阶段2：量化空间生成高分辨率图像</td><td>音频 → 量化代码 → 图像</td><td>在量化空间中训练生成器，提升图像分辨率</td></tr>
<tr><td><strong>StyleSync [18]</strong> 2023</td><td>StyleGAN2生成器</td><td>对抗学习（SyncNet监督）</td><td>单阶段：StyleGAN2生成同步唇部图像</td><td>音频 → 图像</td><td>结合StyleGAN2高保真生成能力与SyncNet监督</td></tr>
<tr><td>VideoReTalking <strong>[8]</strong>   2022</td><td>多组件框架（重演+同步+细化）</td><td>分模块联合训练</td><td>阶段1：语义重演网络<br>阶段2：唇音同步网络<br>阶段3：身份感知细化</td><td>音频 → 图像</td><td>模块化设计分离语义、同步与身份控制</td></tr>
<tr><td><strong>DINet [63]</strong> 2023</td><td>特征图变形网络, 双编码器 + 面部动作单元（AU）系统</td><td>端到端训练</td><td>单阶段：驱动音频直接变形特征图生成嘴型</td><td>音频 → 图像</td><td>通过特征变形实现精细嘴型控制，避免分阶段误差累积</td></tr>
</tbody></table>
<table><thead><tr><th><strong>模型名称</strong></th><th><strong>核心技术</strong></th><th><strong>主要贡献</strong></th><th><strong>关键创新点</strong></th><th><strong>优势</strong></th></tr></thead><tbody>
<tr><td><strong>Wav2Lip [Pra20b]</strong></td><td>预训练唇同步判别器 + 对抗训练</td><td>生成高逼真唇部同步视频</td><td>引入SyncNet作为判别器监督生成器，优化唇-音频对齐</td><td>广泛认可的唇同步效果，适用于多种场景</td></tr>
<tr><td><strong>VideoRetalking [Che22]</strong></td><td>三阶段流程（表情中和→唇同步→身份增强）</td><td>高质量视频编辑的唇同步生成</td><td>分阶段处理（表情中和+身份感知增强），提升身份一致性</td><td>适用于视频编辑，保持人物身份与表情自然</td></tr>
<tr><td><strong>DI-Net [Zha23]</strong></td><td>双编码器 + 面部动作单元（AU）系统</td><td>生成逼真且情感一致的面部视频</td><td>结合面部动作单元系统控制情感表达，双编码器分离内容与身份特征</td><td>情感一致性高，适合需要情绪表达的应用（如虚拟主播）</td></tr>
<tr><td><strong>TalkLip [Wan23]</strong></td><td>对比学习 + Transformer音频编码</td><td>提升唇语音同步的全局时间依赖性</td><td>对比学习优化音频-视频对齐；Transformer建模全局时序关系</td><td>同步效果更精准，适应复杂语音节奏与长时序依赖</td></tr>
</tbody></table>
<hr />
<h2 id="姿势驱动的人类视频生成"><a class="header" href="#姿势驱动的人类视频生成">姿势驱动的人类视频生成</a></h2>
<p>包括单条件姿势引导方法和多条件姿势引导方法。</p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_6_Figure_5_853816112.png" alt="" /></p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_7_Table_IV_-1432891952.png" alt="" /></p>
<h3 id="2d动作驱动"><a class="header" href="#2d动作驱动">2D动作驱动</a></h3>
<p>pose + reference Image -&gt; video</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>108</td><td>2025.4.30</td><td><strong>ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</strong></td><td>参数化的三维物理知识显式地集成到一个预训练的条件视频生成模型中，从而显著增强了其生成高质量、包含复杂动作和交互的视频的能力<br> 1.使用一个视频扩散模型生成一个粗糙的视频 <br> 2. 从该粗略视频中提取一组 2D 和 3D 特征，构建一个以对象为中心的 3D 表示，并通过我们提出的参数化物理先验模型对其进行优化，生成精确的 3D 动作序列。<br> 3. 这一优化后的动作序列被反馈到同一个视频扩散模型中作为额外的条件输入</td><td>三阶段, 即插即用</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/108.html">link</a></td></tr>
<tr><td></td><td>2025.5.6</td><td><strong>FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios</strong></td><td></td><td>姿势引导视频合成</td><td><a href="51.html">link</a></td></tr>
<tr><td></td><td>2025.5.6</td><td>Real-Time Person Image Synthesis Using a Flow Matching Model</td><td></td><td>姿势引导人物图像合成， flow matching</td><td><a href="50.html">link</a></td></tr>
<tr><td>37</td><td>2024</td><td>TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</td><td>通过修正attention map实现背景的时序稳定性</td><td>Diffusion</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/37.html">link</a></td></tr>
<tr><td>2</td><td>2024.1</td><td>Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos</td><td>uses text descriptions to provide semantic information about the content of the characters, ensuring the generated videos align with the textual descriptions.</td><td>人体外观控制<br>设计了一个两阶段训练方案，利用图像姿态对和无姿态视频生成姿态可控的角色动画</td><td><img src="./assets/08-224-2.png" alt="" /></td></tr>
<tr><td></td><td>2023</td><td>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</td><td></td><td></td><td></td></tr>
<tr><td>121</td><td>2023</td><td>MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Dancing avatar: Pose and text-guided human motion videos synthesis with image diffusion model</td><td><img src="../assets/08-224-1.png" alt="" /></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Disco: Disentangled control for referring human dance generation in real world</td><td><img src="./../assets/08-224-3.png" alt="" /></td><td></td><td></td></tr>
</tbody></table>
<h3 id="视频动作驱动"><a class="header" href="#视频动作驱动">视频动作驱动</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>99</td><td>2025.5.19</td><td>FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</td><td>1. 从视频中提取2D pose<br> 2. 2D pose lifting到3D pose<br> 3. <strong>对3D pose作物理优化</strong> <br> 4. 用优化后的pose引导视频生成</td><td><strong>可微的物理优化过程</strong>，pose信息来自视频，无外观信息控制</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/99.html">link</a></td></tr>
<tr><td>53</td><td>2024</td><td>Implicit Warping for Animation with Image Sets</td><td>用driving视频中的人去驱动reference图像中的人，生成reference做与driving中相同动作的视频</td><td>pose信息来自视频<br>外观信息来自Reference Image<br>Cross Attention</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/53.html">link</a></td></tr>
</tbody></table>
<h3 id="3d动作驱动"><a class="header" href="#3d动作驱动">3D动作驱动</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.5.28</td><td>LatentMove: Towards Complex Human Movement Video Generation </td><td></td><td>专门为高度动态的人体动画量身定制的、<strong>基于DiT(扩散Transformer)的框架</strong>的图像到视频(I2V)生成</td><td><a href="135.html">link</a></td></tr>
<tr><td>42</td><td>2024</td><td>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</td><td>3D建模 + 3D重定向 + 渲染，动作控制+相机控制</td><td>人物视频生成，3D管线</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/42.html">link</a></td></tr>
</tbody></table>
<h3 id="虚拟换衣"><a class="header" href="#虚拟换衣">虚拟换衣</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025</td><td>RealVVT: Towards Photorealistic Video Virtual Try-on via Spatio-Temporal Consistency</td><td>虚拟试衣</td><td></td><td></td></tr>
</tbody></table>
<h1 id="数据集和评估指标"><a class="header" href="#数据集和评估指标">数据集和评估指标</a></h1>
<h2 id="数据集"><a class="header" href="#数据集">数据集</a></h2>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_3_Figure_2_-1187442696.png" alt="" /><br />
<img src="./assets/c5094236dee05a597cc12eb2a5b13473_3_Table_II_-1093823805.png" alt="" /></p>
<h2 id="评估指标"><a class="header" href="#评估指标">评估指标</a></h2>
<p><a href="./VideoDiffusionModels/EvaluationMetrics.html">link</a></p>
<h1 id="挑战和难题"><a class="header" href="#挑战和难题">挑战和难题</a></h1>
<ul>
<li>遮挡问题：身体部位重叠或多人遮挡很常见，但大多数模型不能很好地处理相互影响的问题[98]，[138]。</li>
<li>Body Deformation</li>
<li>外观不一致</li>
<li>背景影响</li>
<li>时序不一致</li>
<li>不自然的姿势</li>
<li>文本驱动或语音驱动中，由于本身是一对多问题，可能受限于数据集而存在偏向性</li>
</ul>
<h1 id="影响生成质量的因素"><a class="header" href="#影响生成质量的因素">影响生成质量的因素</a></h1>
<h2 id="生成范式"><a class="header" href="#生成范式">生成范式。</a></h2>
<p>与姿势驱动方法（可以视为一阶段方法）相比，文本和音频驱动方法可以分为一阶段和两阶段方法。前者直接使用输入文本或音频作为提示来指导人类视频生成，而后者从输入文本或音频生成姿势，然后使用这些生成的姿势作为信号来指导人类视频生成。在两阶段方法中引入各种姿势类型（例如骨架姿势）提供了额外的几何和语义信息，从而提高了视频运动的准确性和真实感。这使得两阶段方法明显比一阶段方法更有效，尽管会牺牲一些效率。</p>
<h2 id="backbone"><a class="header" href="#backbone">backbone</a></h2>
<p>SD 和 SVD 等扩散模型因其卓越的性能和多样性而广泛应用于各种生成任务，包括人类视频生成。然而，与在单个采样步骤中生成样本的 GAN 不同，扩散模型需要多个采样步骤，从而增加了训练和推理的时间成本。</p>
<h2 id="pose控制信号"><a class="header" href="#pose控制信号">pose控制信号</a></h2>
<p>不同类型的条件姿势之所以有效，是因为它们提供了补充信息。</p>
<ul>
<li>骨骼姿势准确地描述了帧中人体的空间信息以及身体部位的相对位置。然而，它捕获离散的姿势变化而不是连续的运动细节，提供有限的时间连贯性。</li>
<li>光流本质上包括时间信息，捕获连续帧之间的变化并提供特征空间中的连续运动轨迹。这使得模型能够生成帧之间平滑过渡的视频，避免跳跃或不连续。</li>
<li>深度地图捕捉人体与背景之间的距离信息，以及表面细节和深度变化。</li>
<li>3D 网格提供了骨骼姿势所缺乏的物体表面的详细几何结构。</li>
</ul>
<p>总之，不同类型的姿势提供互补的时空信息，并且不存在满足所有要求的统一姿势类型。不同的场景和问题可能需要不同的姿势。</p>
<h1 id="未来研究方向"><a class="header" href="#未来研究方向">未来研究方向</a></h1>
<ul>
<li>大规模高质量人类视频数据集</li>
<li>长视频生成</li>
<li>高保真视频生成</li>
<li>提高人类视频扩散模型的效率</li>
<li>细粒度可控性</li>
<li>交互性。</li>
</ul>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<ol>
<li><a href="https://arxiv.org/pdf/2407.08428">A Comprehensive Survey on Human Video
Generation: Challenges, Methods, and Insights</a></li>
<li>https://github.com/wentaoL86/Awesome-Human-Video-Generation</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../VideoDiffusionModels/VideoGeneration/Multimodal-guidedgeneration.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../VideoDiffusionModels/VideoEditing.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../VideoDiffusionModels/VideoGeneration/Multimodal-guidedgeneration.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../VideoDiffusionModels/VideoEditing.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../theme/pagetoc.js"></script>
        <script type="text/javascript" src="../theme/mermaid.min.js"></script>
        <script type="text/javascript" src="../theme/mermaid-init.js"></script>
    </body>
</html>
