<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>基于视觉的人类骨骼动作捕捉HPE - ImportantArticles</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Important Articles">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../theme/pagetoc.css">
        <link rel="stylesheet" href="../../theme/custom.css">
        <link rel="stylesheet" href="../../theme/font-sizes.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">动画3D管线 - 3DMesh的驱动</li><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/SkeletonProxy.html"><strong aria-hidden="true">2.</strong> 基于骨骼代理的Mesh的驱动</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/MotionPrior.html"><strong aria-hidden="true">2.1.</strong> 骨骼动作先验</a></li><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/MotionGenerationDiscreteRepresentation.html"><strong aria-hidden="true">2.2.</strong> 基于离散表示的骨骼动作生成</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.</strong> 基于连续表示的骨骼动作生成</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/Locomotion.html"><strong aria-hidden="true">2.3.1.</strong> locomotion</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.2.</strong> 文生动作</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/MotionGeneration/Text2Motion/DiffusionBasedText2Motion.html"><strong aria-hidden="true">2.3.2.1.</strong> 基于Diffusion的文生动作</a></li><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/MotionGeneration/Text2Motion/MambaBasedText2Motion.html"><strong aria-hidden="true">2.3.2.2.</strong> 基于Mamba的文生动作</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../MeshAnimation/SkeletonProxy/HPE_HMR_Summary.html" class="active"><strong aria-hidden="true">2.4.</strong> 基于视觉的人类骨骼动作捕捉HPE</a></li><li class="chapter-item expanded "><a href="../../CharacterAnimation/HumanFacialAnimation.html"><strong aria-hidden="true">2.5.</strong> facial and expression</a></li><li class="chapter-item expanded "><a href="../../CharacterAnimation/HumanMotionGenerationSummary.html"><strong aria-hidden="true">2.6.</strong> Human Motion Generation: A Survey</a></li></ol></li><li class="chapter-item expanded "><a href="../../MeshAnimation/E2E.html"><strong aria-hidden="true">3.</strong> 无代理的Mesh驱动</a></li><li class="chapter-item expanded affix "><li class="part-title">动画3D管线 - Nerf驱动</li><li class="chapter-item expanded "><a href="../../NerfAnimation.html"><strong aria-hidden="true">4.</strong> NerfAnimation</a></li><li class="chapter-item expanded affix "><li class="part-title">动画3D管线 - 3DGS的驱动</li><li class="chapter-item expanded "><a href="../../3DGSAnimation/3DGS.html"><strong aria-hidden="true">5.</strong> 3DGS VS. Nerf</a></li><li class="chapter-item expanded "><a href="../../3DGSAnimation/Dynamic.html"><strong aria-hidden="true">6.</strong> 动态3DGS</a></li><li class="chapter-item expanded "><a href="../../3DGSAnimation/Static.html"><strong aria-hidden="true">7.</strong> 静态3DGS</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../3DGSAnimation/4DReconstruction.html"><strong aria-hidden="true">7.1.</strong> 基于静态3DGS的4D重建</a></li><li class="chapter-item expanded "><a href="../../3DGSAnimation/3DGSAnimation.html"><strong aria-hidden="true">7.2.</strong> 静态3DGS驱动</a></li></ol></li><li class="chapter-item expanded "><a href="../../3D_Gaussian_Splatting.html"><strong aria-hidden="true">8.</strong> A Survey on 3D Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="../../AnimationGeneration.html"><strong aria-hidden="true">9.</strong> Animal Generation</a></li><li class="chapter-item expanded affix "><li class="part-title">动画2D管线 - 像素的驱动，可控视频生成</li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/Introduction.html"><strong aria-hidden="true">10.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/VideoGeneration.html"><strong aria-hidden="true">11.</strong> Video Generation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/Pioneeringearlyworks.html"><strong aria-hidden="true">11.1.</strong> 闭源T2V大模型</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/Open-sourcebasemodels.html"><strong aria-hidden="true">11.2.</strong> 开源T2V基模型</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/WorksBasedOnT2I.html"><strong aria-hidden="true">11.3.</strong> Works Based on T2I 基模型</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/WorksBasedOnT2V.html"><strong aria-hidden="true">11.4.</strong> Works Based on T2V 基模型</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/Storyboard.html"><strong aria-hidden="true">11.5.</strong> Storyboard</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/Longvideogeneration.html"><strong aria-hidden="true">11.6.</strong> Long video generation/Storyboard</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoGeneration/Multimodal-guidedgeneration.html"><strong aria-hidden="true">11.7.</strong> Multimodal-guided generation</a></li><li class="chapter-item expanded "><a href="../../CharacterAnimation/HumanVideoGeneration.html"><strong aria-hidden="true">11.8.</strong> Human Video Generation</a></li></ol></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing.html"><strong aria-hidden="true">12.</strong> Video Editing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing/Tuning-based.html"><strong aria-hidden="true">12.1.</strong> Tuning-based</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing/Training-free.html"><strong aria-hidden="true">12.2.</strong> Training-free</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing/ControlledEditing.html"><strong aria-hidden="true">12.3.</strong> Controlled Editing</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing/3D-Aware.html"><strong aria-hidden="true">12.4.</strong> 3D-Aware</a></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/VideoEditing/OtherGuidance.html"><strong aria-hidden="true">12.5.</strong> Other Guidance</a></li></ol></li><li class="chapter-item expanded "><a href="../../VideoDiffusionModels/EvaluationMetrics.html"><strong aria-hidden="true">13.</strong> 评价指标</a></li><li class="chapter-item expanded affix "><li class="part-title">动画2D管线 - 2D图形的驱动</li><li class="chapter-item expanded "><a href="../../ClipAnimation.html"><strong aria-hidden="true">14.</strong> 2D图形驱动</a></li><li class="chapter-item expanded affix "><li class="part-title">通用AI技术</li><li class="chapter-item expanded "><a href="../../PhysicsSimulation/PINN.html"><strong aria-hidden="true">15.</strong> 2025 PINN Survey</a></li><li class="chapter-item expanded "><a href="../../PhysicsSimulation/Fluid.html"><strong aria-hidden="true">16.</strong> 2024 Fluid Survey</a></li><li class="chapter-item expanded "><a href="../../NeurIPS2024FlowMatchigTurorial/Agenda.html"><strong aria-hidden="true">17.</strong> NeurIPS 2024 Flow Matchig Turorial</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../NeurIPS2024FlowMatchigTurorial/FlowMatchingBasics.html"><strong aria-hidden="true">17.1.</strong> Flow Matching Basics</a></li><li class="chapter-item expanded "><a href="../../NeurIPS2024FlowMatchigTurorial/FlowMatchingAdvancedDesigns.html"><strong aria-hidden="true">17.2.</strong> Flow Matching Advanced Designs</a></li><li class="chapter-item expanded "><a href="../../NeurIPS2024FlowMatchigTurorial/ModelAdaptation.html"><strong aria-hidden="true">17.3.</strong> Model Adaptation</a></li><li class="chapter-item expanded "><a href="../../NeurIPS2024FlowMatchigTurorial/GeneratorMatchingandDiscreteFlows.html"><strong aria-hidden="true">17.4.</strong> Generator Matching and Discrete Flows</a></li></ol></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Introduction.html"><strong aria-hidden="true">18.</strong> Introduction</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">19.</strong> Fundamentals</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Fundamentals/DenoisingDiffusionProbabilisticModels.html"><strong aria-hidden="true">19.1.</strong> Denoising Diffusion Probabilistic Models</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Fundamentals/Score-basedGenerativeModelingwithDifferentialEquations.html"><strong aria-hidden="true">19.2.</strong> Score-based Generative Modeling with Differential Equations</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Fundamentals/AcceleratedSampling.html"><strong aria-hidden="true">19.3.</strong> Accelerated Sampling</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Fundamentals/ConditionalGenerationandGuidance.html"><strong aria-hidden="true">19.4.</strong> Conditional Generation and Guidance</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Fundamentals/Summary.html"><strong aria-hidden="true">19.5.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/Architecture.html"><strong aria-hidden="true">20.</strong> T2I 基模型</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">21.</strong> Image Applications Based on 基模型</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationOnImage/ImageEditing.html"><strong aria-hidden="true">21.1.</strong> 图像生成/编辑</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationOnImage/InverseProblems.html"><strong aria-hidden="true">21.2.</strong> 图像去噪/图像超分/图像补全</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationOnImage/LargeContents.html"><strong aria-hidden="true">21.3.</strong> 大图生成</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">22.</strong> 3D Applications Based on Diffusion</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/2Ddiffusionmodelsfor3Dgeneration.html"><strong aria-hidden="true">22.1.</strong> 基于T2I基模型</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/3D.html"><strong aria-hidden="true">22.2.</strong> 基于不同视角的3D生成</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/Diffusionmodelsforviewsynthesis.html"><strong aria-hidden="true">22.3.</strong> 新视角合成</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/3Dreconstruction.html"><strong aria-hidden="true">22.4.</strong> 3D重建</a></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/Inverseproblems.html"><strong aria-hidden="true">22.5.</strong> 3D编辑</a></li></ol></li><li class="chapter-item expanded "><a href="../../diffusion-tutorial-part/ApplicationsOn3D/Safetyandlimitationsofdiffusionmodels.html"><strong aria-hidden="true">23.</strong> Safety and limitations of diffusion models</a></li><li class="chapter-item expanded "><a href="../../LargeMultimodalModelsNotesonCVPR2023Tutorial.html"><strong aria-hidden="true">24.</strong> Large Multimodal Models Notes on CVPR 2023 Tutorial</a></li><li class="chapter-item expanded "><a href="../../GenerativeModels.html"><strong aria-hidden="true">25.</strong> 生成模型</a></li><li class="chapter-item expanded affix "><li class="part-title">Others</li><li class="chapter-item expanded "><a href="../../数据集.html"><strong aria-hidden="true">26.</strong> 数据集</a></li><li class="chapter-item expanded "><a href="../../More.html"><strong aria-hidden="true">27.</strong> More</a></li><li class="chapter-item expanded affix "><li class="part-title">Views</li><li class="chapter-item expanded "><a href="../../Views/20250903.html"><strong aria-hidden="true">28.</strong> 2025.9.3骨骼动作生成</a></li><li class="chapter-item expanded "><a href="../../Views/20250914.html"><strong aria-hidden="true">29.</strong> 2025.9.14骨骼动作离散编码</a></li><li class="chapter-item expanded "><a href="../../Views/20250920.html"><strong aria-hidden="true">30.</strong> 2025.9.20视频可控生成</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ImportantArticles</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ImportantArticles" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="human-pose-estimation"><a class="header" href="#human-pose-estimation">Human Pose Estimation</a></h1>
<blockquote>
<p>输出关节位置、旋转、连接关系</p>
</blockquote>
<pre class="mermaid">mindmap
基于视觉的人类动作捕捉
    输入信息
        单帧图像/连续视频
        单目/多目
        相机位姿固定/不固定
    输出格式
        SMPL/SMPLX/SMPLH骨骼动作
        自定义骨骼动作
    输出对象
        单人骨骼动作
        多人骨骼动作
        人类骨骼动作 + 物体位姿
        相机位姿
    方法
        特定数据的优化方法
        前向推理方法
    要解决的问题
        动作的连续性
        动作的合理性
        视频数据的摭挡问题与歧义性
        实时流式输出
        与图像数据的一致性
        接触准确且不穿模
        人类动作、相机位姿、人类体型之间的耦合关系
</pre>
<h2 id="单人hpe"><a class="header" href="#单人hpe">单人HPE</a></h2>
<h3 id="图像单人hpe"><a class="header" href="#图像单人hpe">图像单人HPE</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>31</td><td></td><td>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</td><td>基于 SMPL 的 Transformer 框架的HMR</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/31.html">link</a></td></tr>
</tbody></table>
<h4 id="solving-depth-ambiguity"><a class="header" href="#solving-depth-ambiguity">Solving Depth Ambiguity</a></h4>
<h4 id="solving-body-structure-understanding"><a class="header" href="#solving-body-structure-understanding">Solving Body Structure Understanding</a></h4>
<h4 id="solving-occlusion-problems"><a class="header" href="#solving-occlusion-problems">Solving Occlusion Problems</a></h4>
<h4 id="solving-data-lacking"><a class="header" href="#solving-data-lacking">Solving Data Lacking</a></h4>
<h3 id="图像人物-物体交互-hoi"><a class="header" href="#图像人物-物体交互-hoi">图像人物-物体交互 (HOI)</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.4.24</td><td>PICO: Reconstructing 3D People In Contact with Objects</td><td></td><td></td><td><a href="4.html">link</a></td></tr>
</tbody></table>
<h3 id="视频单人hpe"><a class="header" href="#视频单人hpe">视频单人HPE</a></h3>
<h4 id="solving-single-frame-limitation"><a class="header" href="#solving-single-frame-limitation">Solving Single-frame Limitation</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.5.29</td><td>GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion</td><td></td><td>从单目人体视频中生成精确且时序一致的深度图和法线图</td><td><a href="140.html">link</a></td></tr>
</tbody></table>
<h4 id="solving-real-time-problems"><a class="header" href="#solving-real-time-problems">Solving Real-time Problems</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>解决了什么痛点</th><th>主要贡献是什么</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.8.29</td><td>Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</td><td>基于diffusion的方法成本高</td><td>采用分层时序剪枝（HTP）策略，能在保留关键运动动态的同时，从帧级别和语义级别动态剪除冗余姿态令牌。</td><td></td><td></td></tr>
</tbody></table>
<h4 id="solving-body-structure-understanding-1"><a class="header" href="#solving-body-structure-understanding-1">Solving Body Structure Understanding</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td></td><td>Learned Neural Physics Simulation for Articulated 3D Human Pose Reconstruction</td><td>提出替代传统物理引擎的神经网络，辅助视频动作理解。</td><td>LARP</td><td></td></tr>
</tbody></table>
<h4 id="solving-occlusion-problems-1"><a class="header" href="#solving-occlusion-problems-1">Solving Occlusion Problems</a></h4>
<h4 id="solving-data-lacking-1"><a class="header" href="#solving-data-lacking-1">Solving Data Lacking</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>103</td><td>2025.5.2</td><td>GENMO: A GENeralist Model for Human MOtion</td><td>把HPE看作是视频condition的动作生成任务。通过动作估计与动作生成的协同增强，提升动作估计的准确性。</td><td>人体运动通用模型，动作估计，动作生成, NVIDIA</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/103.html">link</a></td></tr>
</tbody></table>
<h2 id="多人hpe"><a class="header" href="#多人hpe">多人HPE</a></h2>
<h1 id="human-mesh-recovery"><a class="header" href="#human-mesh-recovery">Human Mesh Recovery</a></h1>
<h2 id="template-based-human-mesh-recovery"><a class="header" href="#template-based-human-mesh-recovery">Template-based human mesh recovery</a></h2>
<h3 id="naked-human-body-recovery"><a class="header" href="#naked-human-body-recovery">Naked human body recovery</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>解决了什么痛点</th><th>主要贡献是什么</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.8.13</td><td>HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</td><td>1. 传统HPE方法没有考虑特定人体体型与3D姿态的关系，因此牺牲了HPE精度。<br> 2. 依赖2D图像衍生的约束条件的对齐效果来优化姿态。</td><td>1. 通过先校准用户身体形状，再基于该形状进行<strong>个性化</strong>姿态拟合。2. 开发了基于身体形状条件的3D姿态先验模型，有效缓解了因过度依赖2D约束而产生的误差。<br> 升了骨盆对齐姿态精度，还改善了绝对姿态精度</td><td>仅需合成数据训练，即插即用</td><td></td></tr>
</tbody></table>
<h4 id="multimodal-methods"><a class="header" href="#multimodal-methods"><strong>Multimodal Methods</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>[123]</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[124]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[125]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[126]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</td><td></td><td>单人，移动相机</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/11.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</td><td></td><td>2D to 3D lifting</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/27.html">link</a></td></tr>
<tr><td>Moritz Einfalt, Katja Ludwig, and Rainer Lienhart. Uplift and upsample: Efficient 3d human pose estimation with uplifting transformers. In IEEE Winter Conf. Appl. Comput. Vis., pages 2903–2913, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang. Exploiting temporal contexts with strided transformer for 3d human pose estimation. IEEE Trans. Multimedia, 25:1282–1293, 2022a.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation. In Eur. Conf. Comput. Vis., pages 461–478. Springer, 2022.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Junsong Yuan. Mixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video. In IEEE Conf. Comput. Vis. Pattern Recog., pages 13232– 13242, 2022.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Zhenhua Tang, Zhaofan Qiu, Yanbin Hao, Richang Hong, and Ting Yao. 3d human pose estimation with spatio-temporal criss-cross attention. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4790–4799, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen Chen. Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8877–8886, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<h4 id="utilizing-attention-mechanism"><a class="header" href="#utilizing-attention-mechanism">Utilizing Attention Mechanism</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Humans in 4D: Reconstructing and Tracking Humans with Transformers</td><td></td><td>图像，开源</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/28.html">link</a></td></tr>
</tbody></table>
<h4 id="exploiting-temporal-information"><a class="header" href="#exploiting-temporal-information"><strong>Exploiting Temporal Information</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>[134]</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[135]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[136]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[137]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[138]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[139]</td><td>2023</td><td>Global-to-local modeling for video-based 3d human pose and shape estimation</td><td>To effec-tively balance the learning of short-term and long-term temporal correlations, Global-to-Local Transformer (GLoT) [139] structurally decouples the modeling of long-term and short-term correlations.</td><td>视频，单人，SMPL，非流式，transformer</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/12.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Video</td><td>仅图像特征恢复3D动作</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/18.html">link</a></td></tr>
</tbody></table>
<h4 id="multi-view-methods"><a class="header" href="#multi-view-methods">Multi-view Methods.</a></h4>
<h4 id="boosting-efficiency"><a class="header" href="#boosting-efficiency">Boosting Efficiency</a></h4>
<h4 id="developing-various-representations"><a class="header" href="#developing-various-representations">Developing Various Representations</a></h4>
<h4 id="utilizing-structural-information"><a class="header" href="#utilizing-structural-information">Utilizing Structural Information</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>26</td><td>2024.4.5</td><td>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</td><td>利用物理合理化人物动作</td><td>基于SMPL模型从单目视频估计人体动力学，但仅通过拉格朗日损失隐式融入物理约束</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/26.html">link</a></td></tr>
</tbody></table>
<h4 id="choosing-appropriate-learning-strategies"><a class="header" href="#choosing-appropriate-learning-strategies">Choosing Appropriate Learning Strategies</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>161</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>44</td><td>2020</td><td></td><td></td><td></td><td></td></tr>
<tr><td>163</td><td>2020</td><td>Coherent reconstruction of multiple humans from a single image</td><td></td><td>图像，多人</td><td></td></tr>
<tr><td>164</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>46</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>214</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>165</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>166</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>167</td><td>2023</td><td>Jotr: 3d joint con-trastive learning with transformers for occluded human mesh recovery</td><td>融合 2D 和 3D 特征，并通过基于 Transformer 的对比学习框架结合对 3D 特征的监督</td><td></td><td></td></tr>
<tr><td>162</td><td>2023</td><td>Refit: Recurrent fitting network for 3d human recovery</td><td>通过反馈-更新循环机制重新投影关键点并完善人体模型</td><td></td><td></td></tr>
<tr><td>4</td><td>2023</td><td>Co-evolution of pose and mesh for 3d human body estimation from video</td><td>引入了一种利用 3D 姿势作为中介的人体mesh恢复的共同进化方法。该方法将过程分为两个不同的阶段：首先，它从视频中估计 3D 人体姿势，随后，根据估计的 3D 姿势并结合时间图像特征对mesh顶点进行回归</td><td>开源、单人、视频、mesh</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/13.html">link</a></td></tr>
<tr><td>168</td><td>2023</td><td>Cyclic test-time adaptation on monocular video for 3d human mesh reconstruction</td><td>为了弥合训练和测试数据之间的差距，CycleAdapt [168]提出了一种域自适应方法，包括mesh重建网络和运动降噪网络，能够实现更有效的自适应。</td><td></td><td></td></tr>
</tbody></table>
<h3 id="detailed-human-body-recovery"><a class="header" href="#detailed-human-body-recovery">Detailed human body recovery</a></h3>
<h4 id="with-clothes"><a class="header" href="#with-clothes">With Clothes</a></h4>
<h4 id="with-hands"><a class="header" href="#with-hands">With Hands</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>173</td><td>2023</td><td>SGNify, a model that captures hand pose, facial expression, and body movement from sign language videos. It employs linguistic priors and constraints on 3D hand pose to effectively address the ambiguities in isolated signs.</td><td></td><td></td><td></td></tr>
<tr><td>174</td><td>2021</td><td>the relationship between Two- Hands</td><td></td><td></td><td></td></tr>
<tr><td>175</td><td>2021</td><td>the relationship between Hand-Object</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>HMP: Hand Motion Priors for Pose and Shape Estimation from Video</td><td>先用无视频信息的手势数据做手势动作先验。基于先验再做手势识别</td><td>手、开源</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/15.html">link</a></td></tr>
</tbody></table>
<h4 id="whole-body"><a class="header" href="#whole-body">Whole Body</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>176</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>177</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>178</td><td>2021</td><td>independently running 3D mesh recovery regression for face, hands, and body and subsequently combining the outputs through an integration module</td><td></td><td></td><td></td></tr>
<tr><td>179</td><td>2021</td><td>integrates independent es- timates from the body, face, and hands using the shared shape space of SMPL-X across all body parts</td><td></td><td></td><td></td></tr>
<tr><td>180</td><td>2022</td><td>Accurate 3d hand pose estimation for whole-body 3d human mesh estimation</td><td>end-to-end framework for whole-body human mesh recovery named Hand4Whole, which employs joint features for 3D joint rotations to enhance the accuracy of 3D hand predictions</td><td></td><td></td></tr>
<tr><td>181</td><td>2023</td><td>Pymaf-x: Towards well-aligned full-body model regression from monocular images</td><td>to resolve the misalignment issues in regression-based, one-stage human mesh recovery methods by employing a feature pyramid approach and refining the mesh-image alignment parameters.</td><td></td><td></td></tr>
<tr><td>215</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>182</td><td>2023</td><td>One-stage 3d whole-body mesh recovery with component aware transformer</td><td>a simple yet effective component-aware transformer that includes a global body encoder and a lo- cal face/hand decoder instead of separate networks for each part</td><td></td><td></td></tr>
<tr><td>183</td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<h2 id="template-free-human-body-recovery"><a class="header" href="#template-free-human-body-recovery">Template-free human body recovery</a></h2>
<h1 id="运动相机场景"><a class="header" href="#运动相机场景">运动相机场景</a></h1>
<h2 id="提取相机轨迹"><a class="header" href="#提取相机轨迹">提取相机轨迹</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td>BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Decoupling Human and Camera Motion from Videos in the Wild</td><td>联合优化人体姿势和相机scale，使人体位移与学习的运动模型相匹配</td><td>多人</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/16.html">link</a></td></tr>
</tbody></table>
<h1 id="evaluation"><a class="header" href="#evaluation">Evaluation</a></h1>
<h2 id="evaluation-metrics"><a class="header" href="#evaluation-metrics">Evaluation metrics</a></h2>
<h3 id="for-pose-and-shape-reconstruction"><a class="header" href="#for-pose-and-shape-reconstruction">For pose and shape reconstruction</a></h3>
<p>mean per-joint error (MPJPE),   Procrustes-aligned perjoint error (PA-MPJPE),<br />
per-vertex error (PVE)</p>
<h3 id="to-evaluate-the-motion-smoothness"><a class="header" href="#to-evaluate-the-motion-smoothness">To evaluate the motion smoothness</a></h3>
<p>acceleration error (ACCEL) against the ground truth acceleration</p>
<h3 id="for-human-trajectory-evaluation"><a class="header" href="#for-human-trajectory-evaluation">For human trajectory evaluation,</a></h3>
<p>we slice a sequence into 100-frame segments and evaluate 3D joint error after aligning the first two frames (W-MPJPE100) or the entire segment (WA-MPJPE100) [93].<br />
evaluate the error of the entire trajectory after aligning the first frame, with root translation error (RTE), root orientation error (ROE), and egocentric root velocity error (ERVE).</p>
<h3 id="for-camera-trajectory-evaluation"><a class="header" href="#for-camera-trajectory-evaluation">For camera trajectory evaluation</a></h3>
<p>absolute trajectory error (ATE) [75], which performs Procrustes with scaling to align the estimation with ground truth before computing error.</p>
<h3 id="to-evaluate-the-accuracy-of-our-scale-estimation"><a class="header" href="#to-evaluate-the-accuracy-of-our-scale-estimation">To evaluate the accuracy of our scale estimation</a></h3>
<p>evaluate ATE using our estimated scale (ATE-S) [35].</p>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<ol>
<li>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../MeshAnimation/SkeletonProxy/MotionGeneration/Text2Motion/MambaBasedText2Motion.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../../CharacterAnimation/HumanFacialAnimation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../MeshAnimation/SkeletonProxy/MotionGeneration/Text2Motion/MambaBasedText2Motion.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../../CharacterAnimation/HumanFacialAnimation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../../theme/pagetoc.js"></script>
        <script type="text/javascript" src="../../theme/mermaid.min.js"></script>
        <script type="text/javascript" src="../../theme/mermaid-init.js"></script>
    </body>
</html>
