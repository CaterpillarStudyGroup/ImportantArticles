# Face Reenactment and Identity Preservation


# 3D Face Generation and Editing

# Text-to-Face and Style-Based Face Generation

**【翻译】**  
### 可动画头部建模  
参数化3D头部模型作为统计先验被广泛应用于可动画头部建模。3D可变形模型（3DMM）[Paysan等，2009]通过低维主成分表示头部形状。在此基础上，FLAME模型[Li等，2017]引入形状与姿势混合形状（blendshapes），实现了下颌、颈部及眼球的运动控制。后续研究[Daněček等，2022；Feng等，2021，2023]基于参数化头部模型[Blanz与Vetter，2023；Li等，2017；Ploumpis等，2020]进一步建模细节表情与情感。ROME方法[Khakhulin等，2022]提出顶点偏移量以捕捉头发几何，但这些方法因固定拓扑和有限表达能力常产生过度平滑的表面，难以处理头饰或复杂发型等几何结构。另一类研究探索混合表示：DELTA[Feng等，2023]将面部显式网格与NeRF头发建模结合，支持多样化发型。

为实现高质量渲染，多项工作[Gafni等，2021；Grassal等，2022；Xu等，2023]采用神经辐射场（NeRF）[Mildenhall等，2021]建模头部虚拟形象。HeadNeRF[Hong等，2022]提出参数化NeRF模型，将头部模型融入NeRF；INSTA[Zielonka等，2023]基于InstantNGP[Müller等，2022]开发动态NeRF。PointAvatar[Zheng等，2023]提出基于点的表征，通过FLAME表情驱动点云形变场。NeRFBlendshape[Gao等，2022]构建基于NeRF的混合形状模型，结合多级体素场与表情系数实现语义动画控制与超写实渲染。

近期研究[Chen等，2024；Dhamo等，2025；Ma等，2024等]利用3D高斯溅射（3D Gaussian Splatting）[Kerbl等，2023]建模头部形象。FlashAvatar[Xiang等，2024]在网格上附加可学习偏移量的高斯点；GaussianBlendshapes[Ma等，2024]将偏移解耦为混合形状。尽管这些方法对写实形象有效，但难以处理风格化内容。

### 生成式头部建模  

头部建模领域的最新进展利用生成模型合成新视角。PanoHead[An等，2023]采用三网格神经体积表征，支持360度头部合成；Rodin[Wang等，2023b]及其扩展RodinHD[Zhang等，2024]通过扩散模型生成头部三平面图。但这些生成的头部均为静态，无法动画。Liveportrait[Guo等，2024]可将单图动态化为视频，但局限于2D空间。CAT4D[Wu等，2024a]训练多视角可变形扩散模型创建动态形象，但基于扩散的方法常面临跨视角一致性挑战。

另一类研究[Chen等，2023a；Liao等，2024等]通过分数蒸馏采样（SDS）将2D扩散先验提炼至3D，虽能实现高质量，但单形象生成需数小时。相比之下，前馈方法[Hong等，2023；Tang等，2025等]在大规模3D数据集训练后可在秒级生成资产，但因训练数据为通用物体，应用于头部时存在显著领域差距，常产生形状失真。总体而言，现有推理方法仍局限于静态形象重建。

---

**【深度解析】**  
### 技术演进图谱  
| **技术路线**           | **代表性方法**       | **核心突破**                          | **关键局限**                          |
|------------------------|----------------------|---------------------------------------|---------------------------------------|
| **参数化建模**         | 3DMM/FLAME          | 建立可动画的混合形状参数体系          | 拓扑固定导致几何细节缺失              |
| **神经辐射场(NeRF)**   | HeadNeRF/INSTA      | 实现超写实渲染与动态光照              | 难以兼容传统动画管线/高计算成本       |
| **点云与高斯表征**     | PointAvatar/GaussianBlendshapes | 支持非刚性形变的灵活表征          | 风格化内容适应性差/缺乏语义控制       |
| **混合表示**           | DELTA               | 分区优化（面部网格+头发NeRF）         | 接缝区域过渡不自然                    |
| **生成式建模**         | RodIN/PanoHead      | 单图到3D的零样本生成                  | 输出静态/跨视角几何不一致              |

### 关键技术瓶颈突破  
1. **动态-静态表征鸿沟**  
   - 现有生成式方法（如扩散模型）多聚焦静态输出，需通过**时序感知的潜在空间编码**将动画参数（如FACS系数）注入生成过程  
   - 潜在解决方案：在NeRF体积场中嵌入可驱动的形变场（如SE(3)-Field），实现表情驱动的密度场变化  

2. **风格化内容建模**  
   - 传统参数化模型对非写实风格的泛化能力弱，需开发**解耦式风格迁移框架**：  
     - 几何风格（如卡通比例）通过对抗学习在顶点位移空间建模  
     - 外观风格（如赛博朋克色调）通过纹理生成网络实现  

3. **跨模态控制**  
   - 现有方法缺乏多粒度控制接口，理想系统应支持：  
     - **高层语义控制**：通过自然语言描述调整发型（如"蓬松卷发+金属耳环"）  
     - **底层参数控制**：精确调节混合形状权重与骨骼绑定  

该领域正经历从"重建-驱动"到"生成-动画"的范式转换，下一阶段突破将取决于**神经符号系统**（结合生成式AI与参数化建模）与**物理启发生成**（模拟真实肌肉运动）的深度融合。

以下是整理后的表格，概述了文本到人脸生成与编辑模型的关键特性：

---

| **模型名称**         | **基础架构/方法**                              | **主要贡献**                                                                 | **输入**                          | **输出**                  | **训练目标/优化方法**                                                                 | **关键创新点**                                                                 |
|----------------------|-----------------------------------------------|-----------------------------------------------------------------------------|-----------------------------------|---------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| **AdaTrans [32]**    | 非线性潜在空间变换（基于StyleGAN）            | 改进复杂条件编辑能力，保持图像真实感                                        | 潜在代码 + 编辑条件               | 编辑后的面部图像          | 自适应非线性变换优化                                                                | 非线性潜在空间变换替代传统线性编辑（如StyleGAN），提升编辑灵活性                |
| **StyleT2I [33]**    | StyleGAN + CLIP引导                            | 解决属性组合性与生成忠实度问题                                              | 文本描述                          | 符合文本的面部图像        | CLIP-guided对比损失 + 文本到方向模块（Text-to-Direction）                            | 文本到方向模块学习潜在方向；组合属性调整确保多属性正确表达                      |
| **M3Face [34]**      | Muse/VQ-GAN + ControlNet + Imagic优化          | 支持多模态输入（多语言文本、分割掩码、地标）                                | 文本/掩码/地标                    | 多模态编辑的面部图像      | 多模态条件输入融合 + Imagic高保真微调                                                | 端到端集成生成与编辑流程，支持多语言与多模态输入                                |
| **GuidedStyle [35]** | StyleGAN + 知识网络（预训练属性分类器）        | 实现精准、可解释的语义面部编辑                                              | 属性条件（如年龄、表情）          | 属性编辑后的面部图像      | 稀疏注意力控制分层编辑 + 知识网络引导                                                | 稀疏注意力机制实现分层编辑；知识网络防止意外属性变化                            |
| **AnyFace [36]**     | StyleGAN + 两流框架 + CLIP                     | 开放世界自由文本生成，解决模式崩溃与词汇限制                                | 自由文本描述                      | 多样化且对齐文本的面部图像| 跨模态蒸馏（CLIP） + 多样性三元组损失（Diverse Triplet Loss）                       | 两流框架分离合成与重建；跨模态蒸馏增强文本-图像对齐；多样性损失提升生成丰富性    |

|ID|Year|Name|Note|Tags|Link|
|---|---|---|---|---|---|
||2025.5.8|**SOAP: Style-Omniscient Animatable Portraits**|从单张图像生成可动画化的3D虚拟头象| FLAME，FACS面部动作编码，多风格3D头像数据集 |[link](60.md)|
||2025.5.2|Model See Model Do: Speech-Driven Facial Animation with Style Control|| 语音驱动，唇形同步，风格 |[link](41.md)|

---

### **关键说明**  
1. **架构演进**：  
   - **基础模型**：多数基于StyleGAN，逐步引入CLIP、ControlNet等多模态组件。  
   - **编辑方式**：从线性（StyleGAN）→ 非线性（AdaTrans）→ 分层（GuidedStyle）→ 开放世界（AnyFace）。  
2. **多模态支持**：  
   - M3Face支持文本、掩码、地标混合输入，扩展应用场景。  
3. **生成可控性**：  
   - StyleT2I通过文本到方向模块实现语义精准控制；GuidedStyle利用稀疏注意力避免属性干扰。  
4. **开放性与多样性**：  
   - AnyFace通过两流框架与多样性损失，突破传统模型的词汇限制与模式崩溃问题。  

# Speech-Driven and Multimodal Expression Generation

以下是整理后的表格，概述了3D面部动画生成与编辑模型的关键特性：

---

| **模型名称/引用**         | **基础架构/方法**                              | **主要贡献**                                                                 | **输入**                          | **输出**                  | **训练目标/优化方法**                                                                 | **关键创新点**                                                                 |
|--------------------------|-----------------------------------------------|-----------------------------------------------------------------------------|-----------------------------------|---------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| **[37]**  2021               | GPT-2文本编码器 + 扩张卷积音频编码器          | 双模态（音频+文本）驱动，提升上半脸表情与唇同步（优于VOCA [38]/MeshTalk [39]）| 音频 + 文本                       | 3D面部动画                | 联合音频-文本特征对齐                                                               | 首个双模态联合模型，但缺乏头部与视线控制                                        |
| **CSTalk [40]**  2024.4        | Transformer编码器                             | 捕捉面部区域相关性，增强情感语音驱动的动画真实感                            | 情感语音                          | 情感面部动画              | 面部区域关联建模                                                                    | 基于Transformer的跨区域关联编码，但仅支持5种情感                                |
| **ExpCLIP [41]**     2023    | CLIP编码器（文本/图像/表情对齐）               | 支持文本/图像驱动的表情动画，适配多样化情感风格                             | 文本/图像 + 语音                  | 表情丰富的面部动画        | CLIP多模态对齐 + TEAD数据集 + 表情提示增强（Expression Prompt Augmentation）        | 三模态（文本/图像/表情）对齐，扩展情感风格泛化性                                 |
| **[42]**        2023.10         | 解缠表示（风格+内容）                          | 提升身份保持与过渡平滑性，优于FaceFormer [43]的视听同步                     | 语音 + 身份特征                   | 个性化面部动画            | 解缠风格与内容表征                                                                  | 身份保留优化，但计算效率较低                                                    |
| **AdaMesh [44]**  2023.10       | Expression Adapter (MoLoRA) + Pose Adapter    | 个性化语音驱动动画，表达力/多样性/同步性优于GeneFace [45]/Imitator [46]     | 语音 + 个性化参数                 | 个性化表情与姿势动画      | MoLoRA增强的表情适配器 + 基于检索的姿势适配器                                       | 分模块适配表情与姿势，支持高效个性化定制                                        |
| **[47]**      2023           | FaceXHuBERT [48] + FaceDiffuser [49]          | 解耦情感表达与随机运动多样性                                                | 语音 + 情感标签                   | 多样化情感动画            | 随机扩散过程增强运动变化                                                            | 结合HuBERT语音特征与扩散模型，实现可控随机性                                    |
| **NFR [51]**    2023         | 解耦编码（身份码 $z_i$ + 表情码 $z_e$）       | 自动绑定与表情重定向，支持可解释参数（zFACS）                               | 无表情网格 + 目标中性网格          | 重定向后的动画网格        | 身份与表情解耦训练 + 可解释参数生成                                                 | 艺术家友好工具，支持自动绑定与参数化表情控制                                    |

---

### **关键说明**  
1. **多模态驱动**：  
   - **[37]** 和 **ExpCLIP** 通过音频/文本/图像多模态输入增强动画表现力。  
   - **NFR** 专注于网格数据的解耦与重定向，适用于影视与游戏制作。  
2. **个性化与解耦**：  
   - **[42]** 和 **AdaMesh** 通过解缠表示或模块化适配器提升身份保留与个性化控制。  
   - **[47]** 结合扩散模型实现随机运动多样性，平衡可控性与自然性。  
3. **技术挑战**：  
   - 部分模型（如 **[42]**）牺牲计算效率以提升生成质量，需进一步优化实时性。  
   - 情感类型限制（如 **CSTalk** 仅支持5种情感）仍是细分场景应用的瓶颈。  

此表格总结了3D面部动画生成模型的核心技术路径，突出多模态驱动、解耦表示与个性化适配的演进方向。


# Reference

1. Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions