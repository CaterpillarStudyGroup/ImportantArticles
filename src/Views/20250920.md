# 20250914可控视频生成

## 核心问题定义

> 用一句话说清楚：这个技术主要想解决动画/仿真领域的什么经典痛点或瓶颈？

可控视频生成是为了解决视频制作低效的痛点。

## 技术解析 

### 它是什么

> 用直观的语言描述这项技术的核心思想

输出控制条件和参考图像（可以没有），生成特定的视频。  

输入：Text prompt（或其它控制信号）  
输出：video   

### 关键论文

> 关键论文/算法：找到1-2篇最具代表性的开创性论文或关键改进论文。不必深究数学细节，但要看懂其核心架构图和主要贡献。

#### T2I -> T2V

> &#x2705; 由于已有一个开源的大数据文生图预训练模型Stale Diffusion Model。为了充分利用这个预训练模型，通常的做法是**把这个文生图模型改造成文生视频模型**。即，从 2D 输出变成 3D 输出。   
动作信息来源：文本  
外观信息来源：文本

|ID|Year|Name|Note|Tags|Link|
|---|---|---|---|---|---|
|50|2023|Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets|Scaling latent video diffusion models to large datasets<br>**Data Processing and Annotation**||[link](https://caterpillarstudygroup.github.io/ReadPapers/50.html)|

#### T2I/T2V -> TI2V

直接从文本生成视频，很难对视频内容进行更细节的控制，因此演生出了Image-2-Video任务。I2V通常是通过在预训练T2I的基础上，引入reference image的注入和时序层来实现。也可以通过直接在预训练的T2V上增加reference image的注入来实现。

##### 任务1：驱动图像
外观信息来源：图像  
动作信息来源：无控制地续写、或文本



##### 任务2：以视频为控制条件的视频生成
外观信息来源：文本    
动作信息来源：视频

|ID|Year|Name|Note|Tags|Link|
|---|---|---|---|---|---|
|126|2025.7.22|MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation|1. 参考对象（**动作信息来自图像**）与目标对象（**外观信息来自文本**）外观或结构差异显著<br> 2. 显示提取源和目标在外观上的语义匹配以及对应部分的形变关系，通过对源做warp得到目标的大致轮廓，以引作为condition引入视频生成|training-free，开源|

#### T2I/T2V/TI2V + 其它控制信号

选一个合适的（开源）预训练模型，在此基础上
- 注入自己的控制信号，例如图像、控制点、光流、拖拽等
- 构造特定的（相对于训练基模型来说）少量的训练数据
- 根据任务特性引入一些技巧
- 经过（相对于训练基模型来说）少量的训练
就得到了针对特定任务的垂域的视频生成模型。

对于大多数社区玩家来说，只能获取到开源的预训练模型，因此要先了解可用的开源模型。

外观信息来源：图像  
动作信息来源：文本、骨骼动作序列、物理规律、用户交互轨迹等

|ID|Year|Name|Note|Tags|Link|
|---|---|---|---|---|---|
|44|2024|Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling|&#x2705; 用户提供的控制信号（condition）+ Image -> dense光流<br>&#x2705; dense光流（condition） + Image -> 视频|Two-stage，轨迹控制|[link](https://caterpillarstudygroup.github.io/ReadPapers/44.html)|

#### T2V -> Improved T2V

在预训练的T2V的基础上，通过一些微调手段，让它在某些方向更优，成为更强大的基模型

动作信息来源：文本  
外观信息来源：文本

### 所需数据

> 它需要什么样的数据进行训练？（图像、3D模型、运动捕捉数据、仿真数据？）是监督学习、无监督还是自监督？

视频数据，或者控制信号与视频的pair data。

## 应用场景与案例

### 学术界

> 在Siggraph等顶会上，这项技术最常被用在哪些方面？找1-2个论文中的例子。

文生视频模型除了可以用于生成视频，还常常用于辅助其它内容的生成。例如通过先生成角色视频再提取角色动作的方式来生成角色动作。因为视频生成模型用大量真实视频数据训练，包含了视觉先验信息，可以在其它任务训练数据不足的情况下，提供额外的信息。

### 工业界

> 是否有公司已经将其产品化？

| 公司 / 机构 | 模型名称 / 系列 | 核心特点 / 定位 |
| :--- | :--- | :--- |
| **腾讯 (Tencent)** | **混元视频系列** (HunyuanCustom, 图生视频等) | **主体一致性**强，支持多模态控制（如图生视频、音频驱动数字人），并积极开源。 |
| **阿里巴巴 (Alibaba)** | **通义万相** (Wan2.5-preview) | 支持**音画同步**生成，可一次性生成匹配的人声、音效和音乐。 |
| **智谱AI (Zhipu AI)** | **CogVideo** 系列 | 早期代表性中文视频生成模型，后续有**CogVideoX**等升级版本。 |
| **Luma AI** | **Ray3** | 强调具备**推理能力**，可理解复杂指令并进行物理模拟，支持4K HDR视频输出。 |
| **OpenAI** | **Sora** | 生成的视频**逼真度和连贯性**突出，能模拟真实物理世界，但尚未对公众开放。 |
| **Runway** | **Gen** 系列 (如 Gen-3) | 在**影视级质感**和**动态控制**上表现优秀，受到不少视频创作者的青睐。 |
| **谷歌 (Google)** | **Veo** | 与YouTube等产品有深度集成，支持生成**高质量、长时长**的视频。 |
| **Stability AI** | **Stable Video** | 基于其图像生成技术，**开源**是其重要特点，方便开发者研究和定制。 |
| **Meta** | **Make-A-Video** | 依托其庞大的社交数据，致力于从**文本或图像直接生成短视频**。 |
| **字节跳动 (ByteDance)** | **Boximator** | 通过**框选**等精细控制方式，实现对视频中物体运动的精准引导。 |
| **昆仑万维 (Kunlun Wanwei)** | **天工SkyVideo** | 支持文生视频、图生视频等多种模态，致力于生成高质量视频内容。 |

### 电影/VFX：迪士尼、Weta等工作室如何用它？

### 游戏：哪些游戏引擎或大厂在探索它？

### 创业公司：是否有基于该技术的明星创业公司？

## 价值主张分析（战略家的核心思考）

### 效率提升：它能将某个环节的速度提升多少倍？能节省多少艺术家的人力成本？

可快速生成视觉合理的视频，降低视频制作的门槛。

### 质量突破：它是否能实现传统方法无法达到的质量或逼真度？

可生成物理真实的视频，相比于手工绘制视频，更能物理合理。

### 创新可能性：它是否开启了全新的创作范式或产品类型？（例如，实时虚拟制作、个性化内容生成）

可以为其它内容生成技术提供先验。

## 现状与挑战

### 当前局限性：这项技术目前最大的问题是什么？（计算成本高、训练慢、控制力不足、艺术导向性差？）

1. 生成速度慢
2. 不能生成太长的视频

### 未来趋势：它的下一个突破点可能在哪里？

视频的更多可控性编辑。