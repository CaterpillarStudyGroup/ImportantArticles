P12  
# Diffusion Model æ˜¯å¦‚ä½•è¿ä½œçš„ï¼Ÿ


P13   

Denoising diffusion models consist of two processes:    
 - Forward diffusion process that gradually adds noise to input   
 - Reverse denoising process that learns to generate data by denoising    

![](../../assets/D1-13.png) 

P14   
## Forward Diffusion Process

The formal definition of the forward process in T steps:    

### ç›´è§‚ç†è§£

![](../../assets/lhy1-8-1.png)

> çœŸæ­£çš„åŠ å™ªè¿‡ç¨‹ï¼Œ**ä¸æ˜¯ç›´æ¥çš„image + noise**ã€‚  

### ä»æ•°å­¦ä¸Šç†è§£

> &#x2705; ä»ç¬¬ä¸€å¼ å›¾åƒåˆ°æœ€åçš„çº¯å™ªå£°ï¼Œå®é™…ä¸Šæ˜¯åˆ†å¸ƒçš„æ”¹å˜ã€‚

![](../../assets/D1-14.png) 

é€šè¿‡é€æ­¥çš„ scale down è®©å‡å€¼è¶‹è¿‘äº 0ã€‚é€šè¿‡å¼•å…¥å™ªå£°ä½¿æ–¹å·®è¶‹è¿‘äº 1ã€‚ä½¿å¾—åŸå§‹åˆ†å¸ƒé€æ­¥é€¼è¿‘ \\(\mathcal{N} (0,1 )\\)åˆ†å¸ƒï¼Œ   

> &#x2753; æ±‚è”åˆåˆ†å¸ƒæœ‰ä»€ä¹ˆç”¨?    

### ä»æ“ä½œå±‚é¢ç†è§£

![](../../assets/D1-15.png) 

> &#x2705; å®é™…ä¸Šï¼Œåœ¨ç»™å®šä¸€å¼ å›¾åƒx0æ—¶ï¼Œæƒ³è¦è·å¾—ç¬¬tå¼ åŠ å™ªå›¾åƒæ—¶ï¼Œä¸éœ€è¦çœŸçš„é€šè¿‡å…¬å¼\\(q(x_t|x_{t-1})\\)ä» \\(\mathbf{x} _{t-1}\\)åˆ° \\(\mathbf{x} _{t}\\)ä¸€æ­¥ä¸€æ­¥è®¡ç®—å‡ºæ¥ï¼Œå¯ä»¥ç›´æ¥ä» \\(\mathbf{x}_0\\)ç”Ÿæˆä»»æ„çš„ \\(\mathbf{x}_t\\)ã€‚ 

![](../../assets/D1-15-1.png) 

ä»æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ï¼Œä»x0é€æ­¥è®¡ç®—åˆ°xtå’Œä»x0ç›´æ¥è®¡ç®—åˆ°xtï¼Œè¿™ä¸¤ç§è¡Œä¸ºæ˜¯ç­‰ä»·çš„ã€‚  

æ ¹æ®å…¬å¼ \\(\mathbf{x} _t=\sqrt{\bar{a} _t}   \mathbf{x} _0+\sqrt{(1-\bar{a} _t) }  \varepsilon  \\)å¯çŸ¥ï¼Œå½“ \\(\bar{a} _T  â†’ 0\\)ï¼Œåˆ†å¸ƒ\\(q(x_T)\\)çš„å‡å€¼è¶‹äº0ï¼Œæ–¹å·®è¶‹äº1ï¼Œå˜æˆçº¯é«˜æ–¯å™ªå£°ã€‚


P16   
### è¿›ä¸€æ­¥ç†è§£

So far, we discussed the diffusion kernel \\(q(\mathbf{x} _t|\mathbf{x} _0)\\) but what about \\(q(\mathbf{x}_t)\\)?   

![](../../assets/D1-16-1.png) 

The diffusion kernel is Gaussian convolution.    

> &#x2705; convolution æ˜¯ä¸€ç§ä¿¡å·å¹³æ»‘æ–¹æ³•ã€‚    
> &#x2705; \\(q(\mathbf{x} _ t|\mathbf{x} _ 0)\\) æ˜¯æ ‡å‡†é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤ \\(q(\mathbf{x} _ t)\\) æ˜¯ä»¥é«˜æ–¯åˆ†å¸ƒä¸ºæƒé‡çš„çœŸå®æ•°æ®çš„åŠ æƒå¹³å‡ã€‚     

![](../../assets/D1-16-2.png) 


We can sample \\(\mathbf{x}_t \sim q(\mathbf{x}_t)\\) by first sampling \\(\mathbf{x}_0\\) and then sampling \\(\mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_0)\\) (i.e., ancestral sampling).   


> &#x2705; å®é™…ä¸Šï¼Œæ²¡æœ‰ä»»æ„ä¸€ä¸ªæ—¶é—´æ­¥çš„ \\(q(\mathbf{x})\\) çš„çœŸå®åˆ†å¸ƒï¼Œåªæœ‰è¿™äº›åˆ†å¸ƒçš„ sample.    

## Reverse Denoising Process

P17   
### ç›´è§‚ç†è§£

![](../../assets/lhy1-2.png)

![](../../assets/lhy1-4-1.png)

Denoiseæ˜¯ä¸€ä¸ªç½‘ç»œæ¨¡å—ï¼Œé€šè¿‡Denoiseæ¨¡å—å­¦ä¹ æ¯ä¸ªæ—¶é—´æ­¥çš„å»å™ªè¿‡ç¨‹ã€‚  

> &#x2705; æŠŠ \\(\mathbf{x}_0\\) åŠ å™ªä¸º init-noiseï¼Œå†ä» init-noise æ¢å¤å‡º \\(\mathbf{x}_0\\)ï¼Œè¿™ä¸ªæ“ä½œæ˜¯ä¸å¯è¡Œçš„ã€‚     
> &#x2705; å› ä¸ºï¼Œæ ¹æ®å…¬å¼ \\(\mathbf{x} _t=\sqrt{\bar{a} _t}   \mathbf{x} _0+\sqrt{(1-\bar{a} _t) }  \varepsilon  \\), ä¸” \\(\bar{a} _T  â†’ 0\\)ï¼Œé‚£ä¹ˆç»è¿‡ \\(T\\) æ­¥åŠ å™ªåï¼Œ\\(\mathbf{x} _t\approx \varepsilon \\). è€Œæ˜¯ \\(\varepsilon \\) æ˜¯ä¸€ä¸ªä¸ \\(\mathbf{x} _ 0\\) æ²¡æœ‰ä»»åŠ¡å…³ç³»çš„å™ªå£°ï¼Œæ‰€ä»¥ä¸å¯èƒ½ä»ä¸­æ¢å¤å‡º \\(\mathbf{x} _ 0\\).    

### ä»æ•°å­¦ä¸Šç†è§£

ä»xTåˆ°x0çš„è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯åˆ†å¸ƒçš„æ”¹å˜ã€‚ä»\\(\mathcal{N}(\mathbf{x}_Tï¼›\mathbf{0,I})\\)wåˆ†å¸ƒå˜æˆçœŸå®åˆ†å¸ƒçš„è¿‡ç¨‹ã€‚  

![](../../assets/D1-17-2.png) 

![](../../assets/D1-17-1.png) 

ä¸Forwardä¸åŒçš„æ˜¯ï¼Œ\\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\\)æ²¡æœ‰ä¸€ä¸ªå‡†ç¡®çš„æ•°å­¦å…¬å¼æ¥è¡¨è¾¾ã€‚

Can we approximate \\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\\)? Yes, we can use a **Normal distribution** if \\(\beta _t\\) is small in each forward diffusion step.    

> &#x2705; Nomal distribution æ˜¯ç‰¹å®šå‡å€¼å’Œæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒï¼Œä¸ä¸€å®šæ˜¯ std é«˜æ–¯ã€‚  
  
P18    

![](../../assets/D1-18.png) 

å‡è®¾\\(p(\mathbf{x} _ T)\\)å’Œ\\(p(\mathbf{x}_{t-1}|\mathbf{x}_t)\\)åˆ†åˆ«ç¬¦åˆä»¥ä¸Šåˆ†å¸ƒã€‚  
ä»ç¬¬1ä¸ªåˆ†å¸ƒä¸­sampleå‡º\\(x_T\\)ï¼ŒæŠŠå®ƒä»£å…¥ç¬¬äºŒä¸ªåˆ†å¸ƒï¼Œå°±å¯ä»¥sampleå‡º\\(x_{T-1}\\)ï¼Œç›´åˆ°æœ€ç»ˆsampleå‡º\\(x_0\\)

**ç”±äºä»¥ä¸Šæˆªå›¾æ¥è‡ªä¸åŒçš„ææ–™ï¼Œå­˜åœ¨på’Œqæ··æœ‰çš„æƒ…å†µï¼Œéœ€æ³¨æ„åŒºåˆ†ã€‚**

P19   
### Learning Denoising Model   

![](../../assets/D1-19-1.png) 

> &#x2705; ä»¥ä¸Šæ˜¯å»å™ªæ¨¡å‹çš„å…¬å¼ï¼Œä¸‹é¢æœ‰å…³äºè¿™äº›å…¬å¼çš„è¯¦ç»†è§£é‡Šã€‚  

P20   
# è®­ç»ƒä¸æ¨æ–­

ä½¿ç”¨Forwardæµç¨‹å¯¹çœŸå®æ•°æ®åŠ å™ªï¼Œä»¥æ„é€ pair dataã€‚  
ä½¿ç”¨ä½¿ç”¨Denoiseæ¨¡å—å­¦ä¹ å»å™ªåˆ†å¸ƒï¼Œå®Œæˆå»å™ªè¿‡ç¨‹ã€‚  

![](../../assets/lhy1-5.png)

![](../../assets/D1-20.png) 

P21    
## Implementation Considerations   

Diffusion models often use U-Net architectures with ResNet blocks and self-attention layers to represent \\(\epsilon _\theta (\mathbf{x}_t,t)\\).    

![](../../assets/D1-21.png) 

Time representation: sinusoidal positional embeddings or random Fourier features.    

Time features are fed to the residual blocks using either simple spatial addition or using adaptive group normalization layers. (see <u>Dharivwal and Nichol NeurIPS 2021</u>).    

> &#x2705; \\(\sigma \\) æ˜¯æ€ä¹ˆå®šä¹‰çš„ï¼Ÿ    

# æ•°å­¦åŸç†


P10   
## ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šçš„å…±åŒç›®æ ‡

### ç›®æ ‡æ˜¯è¦å­¦ä¸€ä¸ªåˆ†å¸ƒ

![](../../assets/lhy3-10.png) 

ç”Ÿæˆæ¨¡å‹çš„æœ¬è´¨æ˜¯è¦å­¦åˆ°çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œä»¥åŠä»æŸä¸ªå·²ç»åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒï¼‰åˆ°è¿™ä¸ªçœŸå®æ•°æ®åˆ†å¸ƒçš„æ˜ å°„ã€‚  

> &#x2705; å®é™…ä½¿ç”¨ä¸­è¿˜ä¼šåŠ ä¸€ä¸ª conditionï¼Œä½†æ•´ä½“ä¸Šæ²¡æœ‰æœ¬è´¨å·®å¼‚ï¼Œå› æ­¤åé¢æ¨å¯¼ä¸­ä¸è€ƒè™‘ condition.    



P11   
### å®šä¹‰ç›®æ ‡å‡½æ•°

#### ä»¥Minimize KL Divergenceä½œä¸ºç›®æ ‡å‡½æ•°

![](../../assets/lhy3-11.png) 

ç›®æ ‡æ˜¯è®©ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒä¸çœŸå®æ•°æ®çš„åˆ†å¸ƒå°½é‡çš„æ¥è¿‘ï¼Œä½†æ˜¯æ€æ ·è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒæ˜¯å¦æ¥è¿‘ï¼Ÿ   

> &#x2705; å¸¸ç”¨KL Divergenceæ¥è¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸GTåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚

#### ä»¥Maximum Likelihood Estimation

\\(P_{data}\\) ä»£è¡¨çœŸå®åˆ†å¸ƒï¼Œä»åˆ†å¸ƒä¸­ Sample å‡ºæ¥çš„ \\(x\\) å³è®­ç»ƒé›†    
\\(x_i\\)æ˜¯æ•°æ®é›†é‡Œçš„ä¸€ä¸ªæ•°æ®ï¼Œä¹Ÿæ˜¯çœŸå®æ•°æ®åˆ†å¸ƒé‡Œçš„ä¸€ä¸ªé‡‡æ ·ã€‚\\(P_\theta (x^i)\\) ä»£è¡¨ \\(P_\theta\\) ç”Ÿæˆ \\(x^i\\) çš„æ¦‚ç‡ã€‚  

> &#x2705; ç”±äº \\(P_\theta\\) éå¸¸å¤æ‚ï¼Œç®—ä¸å‡ºè¿™ä¸ªæ¦‚ç‡ï¼Œä½†æ­¤å¤„å‡è®¾ \\(P_\theta (x^i)\\) å·²çŸ¥ã€‚    

äºæ˜¯å¯ä»¥å°†å®šä¹‰ç›®æ ‡å‡½æ•°ä¸ºï¼šæ‰¾å‡ºè®©çœŸå® \\(x^i\\) è¢«ç”Ÿæˆå‡ºæ¥çš„æ¦‚ç‡æœ€é«˜çš„\\(\theta \\).    

\begin{align*} \theta ^\ast =\text{arg } \max_{\theta } \prod_{i=1}^{m} P_\theta (x^i) \end{align*}

#### ä¸¤ä¸ªç›®æ ‡å‡½æ•°æ˜¯ç­‰ä»·çš„

å¯é€šè¿‡æ•°æ®æ¨å¯¼è¯æ˜ï¼Œè¿™é‡Œæåˆ°çš„ä¸¤ä¸ªç›®æ ‡ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚è¯æ˜è¿‡ç¨‹å¦‚ä¸‹ï¼š

P12  

![](../../assets/lhy3-12.png) 


Maximum Likelihood = Minimize KL Divergence    

> &#x2705; ç»“è®ºï¼šè®©çœŸå®æ•°æ®çš„æ¦‚ç‡æœ€å¤§ï¼Œä¸è®©ä¸¤ä¸ªåˆ†å¸ƒå°½é‡æ¥è¿‘ï¼Œåœ¨æ•°å­¦ä¸Šæ˜¯ä¸€è‡´çš„ã€‚   
> &#x2705; VAEã€diffusionã€flow based ç­‰ç”Ÿæˆæ¨¡å‹ï¼Œéƒ½æ˜¯ä»¥æœ€å¤§åŒ– Likelihood ä¸ºç›®æ ‡ã€‚GAN æ˜¯æœ€å°åŒ– JS Divergence ä¸ºç›®æ ‡ã€‚   


P13   
## Compute \\(ğ‘ƒ_\theta(x)\\)   

### è®¡ç®—\\(ğ‘ƒ_\theta(x)\\)çš„å¸¸ç”¨æŠ€å·§

> &#x2705; VAE å’Œ diffusion éå¸¸ç›¸ä¼¼ï¼Œè®¸å¤šå…¬å¼æ˜¯é€šç”¨çš„ã€‚    

#### æŠ€å·§ä¸€ï¼šä¸æ¨æ–­ç”Ÿæˆç»“æœï¼Œè€Œæ˜¯æ¨æ–­ç”Ÿæˆç»“æœåˆ†å¸ƒçš„å‡å€¼

|||
|--|--|
| ![](../../assets/lhy3-13-1.png) | ![](../../assets/lhy3-13-2.png) |
| ![](../../assets/lhy3-13-3.png) | ![](../../assets/lhy3-13-4.png) |


> &#x2705; \\(Gï¼ˆzï¼‰\\) ä¸ä»£è¡¨æŸä¸ªç”Ÿæˆç»“æœï¼Œè€Œæ˜¯ä¸€ä¸ªé«˜æ–¯çš„å‡å€¼ï¼Œç„¶åè®¡ç®— \\(x\\) åœ¨è¿™ä¸ªåˆ†å¸ƒä¸­çš„æ¦‚ç‡ã€‚   


P14   
#### æŠ€å·§äºŒï¼šä¸æ±‚\\(ğ‘ƒ_\theta(x)\\)ï¼Œè€Œæ˜¯æ±‚Lower bound of \\(log P(x)\\)  

![](../../assets/lhy3-14.png)


> &#x2705; é€šå¸¸æ— æ³•æœ€å¤§åŒ– \\(Pï¼ˆxï¼‰\\)ï¼Œè€Œæ˜¯æœ€å¤§åŒ– \\(log P(x)\\) çš„ä¸‹ç•Œã€‚  
> &#x2705; ä»¥ä¸Šå…¬å¼æ¨å¯¼ä¸­çœç•¥å‚æ•° \\( \theta\\)ã€‚   


P15   
### DDPM: Compute \\(ğ‘ƒ_\theta(x)\\)   

å¯¹äº diffusion modelï¼Œå‡è®¾æ¯æ¬¡ denoise å‡ºçš„æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ã€‚   

> &#x2753; é—®ï¼šä¸ºä»€ä¹ˆå‡è®¾\\(G(x_t)\\) æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ meanï¼Ÿ   
> &#x2705; ç­”ï¼šæœ‰äººå°è¯•è¿‡å…¶å®ƒå‡è®¾ï¼Œæ•ˆæœæ²¡æœ‰å˜å¥½ï¼Œä¸”é«˜æ–¯åˆ†å¸ƒä¾¿äºè®¡ç®—ã€‚   

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œå¯ä»¥å¾—å‡º \\(x_0\\) åœ¨æœ€ç»ˆåˆ†å¸ƒä¸­çš„æ¦‚ç‡ä¸ºï¼š

$$
P_ \theta (x_0)=\int\limits _ {x_1:x_T}^{} P(x_T)P_ \theta (x_{T-1}|x_T) \dots P_ \theta (x_ {t-1}|x_t) \dots P_ \theta(x_0|x_1)dx_1:x_T  
$$

P16   
### DDPM: Lower bound of \\(log P(x)\\)  

![](../../assets/lhy3-16-1.png)  

![](../../assets/lhy3-16-2.png)  

### è®¡ç®—Lower bound of \\(log P(x)\\)

#### è®¡ç®—\\(qï¼ˆx_tï½œx_{t-1}ï¼‰\\)

P17   
![](../../assets/lhy3-17.png)  

> &#x2705; æå‰å®šå¥½ä¸€ç»„ \\(\beta \\)ï¼ä»£è¡¨ noise è¦åŠ å¤šå¤§ã€‚   
> &#x2705; \\(qï¼ˆx_tï½œx_{t-1}ï¼‰\\) ä»ç„¶å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸º \\(\sqrt{1-\beta _t} \cdot x_t\\)ï¼Œæ–¹å·®ä¸º \\(\beta _t\\).   

#### è®¡ç®—\\(qï¼ˆx_tï½œx_{0}ï¼‰\\)

P18   
![](../../assets/lhy3-18.png)  

P19   
![](../../assets/lhy3-19.png)  


> &#x2705; ç”±äºä¸¤æ¬¡ sample å‡ºçš„ noise æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œä¸¤ä¸ª noise ä»¥è¿™ç§å½¢å¼ç›¸åŠ çš„ç»“æœï¼Œä¹Ÿç¬¦åˆæŸä¸ªç‰¹å®šçš„é«˜æ–¯åˆ†å¸ƒã€‚   

P20   

![](../../assets/lhy3-20.png)  

> &#x2705; ç»“è®ºï¼š\\(qï¼ˆx_tï½œx_{0}ï¼‰\\)ä¹Ÿç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸º\\(\bar{\alpha }_t\\)ï¼Œæ–¹å·®ä¸º\\({1-\bar{\alpha }_t}\\).    

## å®šä¹‰æŸå¤±å‡½æ•°

å¦‚ä½•å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¯ä»¥è¾¾åˆ°æœ€å¤§åŒ–\\(\log P_{\theta}(x_0)\\)çš„ç›®çš„

### æŸå¤±å‡½æ•°ä¸ç›®æ ‡å‡½æ•°

> ç›®æ ‡å‡½æ•°æ˜¯æ ¹æ®å®é™…æ„ä¹‰æ¨å¯¼å‡ºæ¥çš„ä¼˜åŒ–ç›®æ ‡ã€‚æŸå¤±å‡½æ•°æ˜¯èƒ½å¼•å¯¼å­¦ä¹ æ”¶æ•›åˆ°ç›®æ ‡çŠ¶æ€çš„å‡½æ•°ï¼Œå¯ä»¥æ²¡æœ‰å®é™…æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥è·Ÿç›®æ ‡å‡½æ•°ä¸ä¸€æ ·ã€‚  
> è™½ç„¶ç›®æ ‡å‡½æ•°å¾ˆæ˜ç¡®ï¼Œä½†æ˜¯æŸå¤±å‡½æ•°ä¸ä¸€å®šè¦è·Ÿç›®æ ‡å‡½æ•°ä¸€æ ·ã€‚å¯ä»¥ä»ç›®æ ‡å‡½æ•°ä¸­æå–å‡ºå½±å“ç»“æœçš„å…³é”®å› ç´ æ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚  

### æ¨å¯¼ä¸ç®€åŒ–ç›®æ ‡å‡½æ•°\\(log P(x)\\)

![](../../assets/lhy3-16-2.png)  

P21   
![](../../assets/lhy3-21.png)  

P22   

æœ€åç®€åŒ–ä¸ºä»¥ä¸‹ä¸‰é¡¹ï¼š  

\begin{align*} E_{q(x_1|x_0)}[log P(x_0|x_1)]-KL(q(x_T|x_0)||P(x_T))
-\sum_{t=2}^{T}E_{q(x_t|x_0)}[KL(q(x_{t-1}|x_t,x_0)||P(x_{t-1}|x_t))]   \end{align*}

### åˆ†æç›®æ ‡å‡½æ•°ä¸­ä¸ä¼˜åŒ–ç›¸å…³çš„å…³é”®å› ç´ 

#### ç»“è®º

> &#x2705; ç›®æ ‡æ˜¯è¦ä¼˜åŒ– \\( \theta\\)ï¼Œç¬¬äºŒé¡¹ä¸\\( \theta\\)æ— å…³ï¼Œå¯ä»¥ç•¥æ‰ã€‚   
> &#x2705; ç¬¬ä¸‰é¡¹çš„ KL Divrgence æ¶‰åŠåˆ°ä¸¤ä¸ªåˆ†å¸ƒï¼Œåˆ†å¸ƒ1æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å¾—åˆ°ï¼Œåˆ†å¸ƒ2æ˜¯ç”± \\( \theta\\) å†³å®šçš„ï¼Œæ˜¯è¦ä¼˜åŒ–çš„å¯¹è±¡ã€‚    

P23  

#### å…³äºç¬¬ä¸‰é¡¹åˆ†å¸ƒ1çš„æ¨å¯¼è¿‡ç¨‹

![](../../assets/lhy3-23-1.png)  

å·²çŸ¥ \\(q (x_t\mid x_0)\\)ï¼Œ\\(q (x_{t-1} \mid x_0)\\) å’Œ \\(q (x_t \mid x_{t-1})\\)ä¸ºï¼š

![](../../assets/lhy3-23-2.png)  

æ±‚ \\(q (x_{t-1} \mid x_t,x_0)\\).   

> &#x2705; \\((q(x_{t-1}|x_t,x_0)\\)çš„æ•°æ®å«ä¹‰ä¸ºï¼šå·²çŸ¥\\(x_0\\) å’Œ \\(x_t\\)ï¼Œæ±‚ \\(x_{t-1}\\) çš„åˆ†å¸ƒã€‚


P24   
![](../../assets/lhy3-24.png)  

P25   
![](../../assets/lhy3-25.png)  

> https://arxiv.org/pdf/2208.11970.pdf


P26   
![](../../assets/lhy3-26.png)  

> &#x2705; ç»“è®ºï¼š\\(q(x_{t-1}|x_t,x_0)\\) ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œä¸”å…¶å‡å€¼ä¸æ–¹å·®æ˜¯ä¸\\(\theta\\)æ— å…³çš„å›ºå®šçš„å€¼ã€‚   

#### åŒ–ç®€åçš„ç›®æ ‡å‡½æ•°

æ ¹æ®ä»¥ä¸Šæ¨å¯¼ï¼Œç›®æ ‡å‡½æ•°å¯ç®€åŒ–ä¸ºæœ€å°åŒ–åŸç›®æ ‡å‡½æ•°ç¬¬ä¸‰é¡¹ä¸­åˆ†å¸ƒ1ä¸åˆ†å¸ƒ2çš„KL Divergenceã€‚  

\begin{align*} E_{q(x_1|x_0)}[log P(x_0|x_1)]-KL(q(x_T|x_0)||P(x_T))
-\sum_{t=2}^{T}E_{q(x_t|x_0)}[KL(q(x_{t-1}|x_t,x_0)||P(x_{t-1}|x_t))]   \end{align*}

å…¶ä¸­åˆ†å¸ƒ1ä¸ºä¸\\(\theta\\)æ— å…³çš„å›ºå®šï¼Œåˆ†å¸ƒ2ä¸ºä¸\\(\theta\\)æœ‰å…³çš„å¾…ä¼˜åŒ–åˆ†å¸ƒã€‚  

#### How to minimize KL divergence?    

##### æ–¹å¼ä¸€ï¼šç›´æ¥å¥—å…¬å¼

![](../../assets/lhy3-27-3.png)  

> &#x2705; ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ KLD æœ‰å…¬å¼è§£ï¼Œä½†æ­¤å¤„ä¸ç”¨å…¬å¼è§£ï¼Œå› ä¸º  \\( \theta\\) åªèƒ½å½±å“åˆ†å¸ƒ2çš„å‡å€¼ã€‚   

##### æ–¹å¼äºŒ

åˆ†å¸ƒ1çš„å‡å€¼å’Œæ–¹å·®æ˜¯å›ºå®šçš„ã€‚åˆ†å¸ƒ2çš„å‡å€¼æ˜¯å¾…ä¼˜åŒ–çš„ï¼Œæ–¹å·®æ˜¯å›ºå®šçš„ã€‚  

![](../../assets/lhy3-27-2.png)  

> &#x2705; å› æ­¤å‡å° KLD çš„æ–¹æ³•æ˜¯è®©åˆ†å¸ƒ2çš„å‡å€¼æ¥è¿‘åˆ†å¸ƒ1çš„å‡å€¼ã€‚   

### å®šä¹‰æŸå¤±å‡½æ•°

> &#x2705; åˆ†å¸ƒ1çš„å‡å€¼å¯ä»¥çœ‹ä½œæ˜¯ \\(x_{t-1}\\) çš„ GT äº†ã€‚å…¶è®¡ç®—å…¬å¼ä¸ºï¼š     

![](../../assets/lhy3-28-2.png)  

\\(x_{t-1}\\)çš„GTçš„è®¡ç®—å…¬å¼ä¸­åŒ…å«äº†x0å’Œxtï¼ŒæŠŠx0å’Œxtéƒ½è½¬åŒ–ä¸ºxtçš„è¡¨ç¤ºï¼Œå¾—ï¼š  

![](../../assets/lhy3-31.png)  

> &#x2705; å¯ä»¥å‘ç° \\(x_t\\) ä¸ \\(x_{t-1}\\)å’ŒGT ä¹‹é—´ï¼Œå”¯ä¸€æœªçŸ¥çš„éƒ¨åˆ†å°±æ˜¯ noise \\(\varepsilon \\). å› æ­¤ç”¨ç½‘ç»œå­¦ä¹ è¿™ä¸ªnoiseã€‚  

æœ€ç»ˆå®šä¹‰æŸå¤±å‡½æ•°ä¸ºç½‘ç»œè¾“å‡º(é¢„æµ‹çš„noise)ä¸GTï¼ˆæ„é€ è®­ç»ƒæ•°æ®æ—¶æ‰€ç”Ÿæˆçš„noiseï¼‰ä¹‹é—´çš„L2è·ç¦»ã€‚  

## å…¶å®ƒé—®é¢˜

### å…³äº\\(\alpha \\)

> &#x2705; \\(\alpha \\) æ˜¯é¢„å®šä¹‰çš„è¶…å‚ï¼ŒDDPM è¯•å›¾å­¦ä¹  \\(\alpha \\)ï¼Œå‘ç°æ²¡æœ‰æå‡ã€‚

# ç›¸å…³è®ºæ–‡ 

|ID|Year|Name|Note|Tags|Link|
|---|---|---|---|---|---|
||2015|Deep Unsupervised Learning using Nonequilibrium Thermodynamics|     
||2020|Denoising Diffusion Probabilistic Models|   

---------------------------------------
> æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
>
> https://caterpillarstudygroup.github.io/ImportantArticles/